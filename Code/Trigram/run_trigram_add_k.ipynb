{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re, json, time, unittest\n",
    "import itertools, collections\n",
    "from importlib import reload\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "\n",
    "# Helper libraries for this notebook\n",
    "from common import utils, vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trigram_addk import next_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /Users/nathanielschub/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "Vocabulary size: 29,598\n",
      "Most common unigrams:\n",
      "\".\": 94,687\n",
      "\",\": 72,360\n",
      "\"the\": 69,277\n",
      "\"DGDG\": 48,960\n",
      "\"DG\": 46,478\n",
      "\"DGDGDG\": 40,589\n",
      "\"of\": 36,779\n",
      "\"to\": 36,400\n",
      "\"in\": 29,253\n",
      "\"and\": 25,648\n",
      "Loaded 54,716 sentences (1.72092e+06 tokens)\n",
      "Training set: 43,772 sentences (1,383,234 tokens)\n",
      "Test set: 10,944 sentences (337,683 tokens)\n",
      "Train set vocabulary: 26,822 words\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('reuters')\n",
    "corpus = nltk.corpus.reuters\n",
    "token_feed = (utils.canonicalize_word(w) for w in corpus.words())\n",
    "vocab = vocabulary.Vocabulary(token_feed)\n",
    "V=30000\n",
    "print(\"Vocabulary size: {:,}\".format(vocab.size))\n",
    "print(\"Most common unigrams:\")\n",
    "for word, count in vocab.unigram_counts.most_common(10):\n",
    "    print(\"\\\"{:s}\\\": {:,}\".format(word, count))\n",
    "\n",
    "train_sents, test_sents = utils.get_train_test_sents(corpus, split=0.8, shuffle=False)\n",
    "vocab = vocabulary.Vocabulary((utils.canonicalize_word(w) for w in utils.flatten(train_sents)), size=V)\n",
    "print(\"Train set vocabulary: {:,} words\".format(vocab.size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1544239\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import reuters\n",
    "a= reuters.raw().lower()\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "#import re\n",
    "a = word_tokenize(a)\n",
    "#tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#tokenizer.tokenize(a)\n",
    "#a = a.split()\n",
    "print(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['asian', 'exporters', 'fear', 'damage', 'from', 'u.s.-japan', 'rift', 'mounting', 'trade', 'friction', 'between', 'the', 'u.s.', 'and', 'japan', 'has', 'raised', 'fears', 'among', 'many', 'of', 'asia', \"'s\", 'exporting', 'nations', 'that', 'the', 'row', 'could', 'inflict', 'far-reaching', 'economic', 'damage', ',', 'businessmen', 'and', 'officials', 'said', '.', 'they', 'told', 'reuter', 'correspondents', 'in', 'asian', 'capitals', 'a', 'u.s.', 'move', 'against', 'japan', 'might', 'boost', 'protectionist', 'sentiment', 'in', 'the', 'u.s.', 'and', 'lead', 'to', 'curbs', 'on', 'american', 'imports', 'of', 'their', 'products', '.', 'but', 'some', 'exporters', 'said', 'that', 'while', 'the', 'conflict', 'would', 'hurt', 'them', 'in', 'the', 'long-run', ',', 'in', 'the', 'short-term', 'tokyo', \"'s\", 'loss', 'might', 'be', 'their', 'gain', '.', 'the', 'u.s.', 'has', 'said', 'it']\n"
     ]
    }
   ],
   "source": [
    "print(a[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = a\n",
    "wordset = set()\n",
    "\n",
    "counts = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "context_totals = dict()\n",
    "w_1, w_2 = None, None\n",
    "for word in words:\n",
    "    wordset.add(word)\n",
    "    if w_1 is not None and w_2 is not None:\n",
    "        counts[(w_2,w_1)][word] += 1\n",
    "            # Update context\n",
    "    w_2 = w_1\n",
    "    w_1 = word\n",
    "for key, value in counts.items():\n",
    "    context_totals[key] = sum(value.values())\n",
    "    \n",
    "words = list(wordset)\n",
    "V = len(words)\n",
    "k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function <lambda>.<locals>.<lambda> at 0x1a18a2a620>, {'where': 1.0, ',': 5.0, 'and': 1.0, 'agriculture': 1.0, 'in': 2.0, 'to': 1.0, 'economy': 1.0, 'responded': 2.0, 'later': 1.0, '.': 2.0})\n"
     ]
    }
   ],
   "source": [
    "print(counts[('the', 'u.s')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_proba(corpus_tokenized, x, context, k):\n",
    "    counts = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "    context_totals = dict()\n",
    "    w_1, w_2 = None, None\n",
    "    wordset = set()\n",
    "    for word in corpus_tokenized:\n",
    "        wordset.add(word)\n",
    "        if w_1 is not None and w_2 is not None:\n",
    "            counts[(w_2,w_1)][word] += 1\n",
    "            # Update context\n",
    "        w_2 = w_1\n",
    "        w_1 = word\n",
    "    for key, value in counts.items():\n",
    "        context_totals[key] = sum(value.values())\n",
    "    \n",
    "    words = list(wordset)\n",
    "    V = len(words)\n",
    "    print(V)    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    C_abc = 0.0\n",
    "    if counts.get(context, 0.0) == 0.0:\n",
    "        C_abc = 0.0\n",
    "    else:\n",
    "        C_abc = counts.get(context, 0.0).get(x, 0.0)\n",
    "                \n",
    "    C_ab = context_totals.get(context, 0.0)\n",
    "        \n",
    "    return (C_abc +k)/(C_ab +(k*V))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.2675994058889557e-05"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_proba(a,'economy',('the','u.s'), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2727272727272727"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = ['my', 'name', 'is', 'nat', 'test', 'test', 'test']\n",
    "\n",
    "next_proba(b, 'test', ('test','test'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_candidates(corpus_tokenized, list_target_words, context, k):\n",
    "    counts = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "    context_totals = dict()\n",
    "    w_1, w_2 = None, None\n",
    "    wordset = set()\n",
    "    for word in corpus_tokenized:\n",
    "        wordset.add(word)\n",
    "        if w_1 is not None and w_2 is not None:\n",
    "            counts[(w_2,w_1)][word] += 1\n",
    "            # Update context\n",
    "        w_2 = w_1\n",
    "        w_1 = word\n",
    "    for key, value in counts.items():\n",
    "        context_totals[key] = sum(value.values())\n",
    "    \n",
    "    words = list(wordset)\n",
    "    V = len(words)\n",
    "    \n",
    "    values=[]\n",
    "    \n",
    "    for word_x in list_target_words:\n",
    "    \n",
    "        C_abc = 0.0\n",
    "        if counts.get(context, 0.0) == 0.0:\n",
    "            C_abc = 0.0\n",
    "        else:\n",
    "            C_abc = counts.get(context, 0.0).get(word_x, 0.0)\n",
    "\n",
    "        C_ab = context_totals.get(context, 0.0)\n",
    "\n",
    "        values.append((word_x,(C_abc +k)/(C_ab +(k*V)) )) \n",
    "    return values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('test', 0.2727272727272727), ('tst', 0.18181818181818182)]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_candidates(b, ['test','tst'],('test','test'),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('where', 2.8342261145594194e-05),\n",
       " (',', 6.613194267305312e-05),\n",
       " ('agriculture', 2.8342261145594194e-05),\n",
       " ('economy', 2.8342261145594194e-05)]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_candidates(a, ['where',',','agriculture', 'economy'], ('the','u.s'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
