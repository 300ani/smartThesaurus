{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim                     # implements word2vec model infrastructure and provides interfacing APIs \n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.test.utils import get_tmpfile\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "import os\n",
    "from scipy import spatial\n",
    "from collections import defaultdict\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "import datetime\n",
    "import timeit \n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained word2vec model\n",
    "word2vec_vectors = '../pretrained/GoogleNews-vectors-negative300.bin'\n",
    "w2v = gensim.models.KeyedVectors.load_word2vec_format(word2vec_vectors, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thisWord = 'major'\n",
    "thisSentence = 'there is going to be a major change in the organization'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar 20 words (by word) for 'major' by word2vec model:\n",
      "['biggest', 'significant', 'big', 'main', 'key', 'huge', 'signficant', 'amajor', 'largest', 'greatest', 'Major', 'massive', 'minor', 'substantial', 'monumental', 'notable', 'signifcant', 'big_gest', 'MAJOR', 'leading']\n"
     ]
    }
   ],
   "source": [
    "# most similar words - by word\n",
    "n_similar = 20\n",
    "synonyms = w2v.similar_by_word(thisWord, n_similar)\n",
    "\n",
    "synonyn_candidate = []\n",
    "for words in synonyms:\n",
    "    synonyn_candidate.append(words[0])\n",
    "\n",
    "print(\"Most similar {} words (by word) for '{}' by word2vec model:\".format(n_similar, thisWord))\n",
    "print(synonyn_candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is going to be a major change in the organization\n",
      "major  :  JJ\n"
     ]
    }
   ],
   "source": [
    "sentence_pos_tag = nltk.pos_tag(thisSentence.split(' '))\n",
    "word_loc = 0\n",
    "word_pos = ''\n",
    "for word_idx, word in enumerate(sentence_pos_tag):\n",
    "    if(word[0] == thisWord):\n",
    "        word_loc = word_idx\n",
    "        word_pos = word[1]\n",
    "\n",
    "print (thisSentence)\n",
    "print (thisWord,' : ', word_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is going to be a major change in the organization\n",
      "biggest  :  JJS\n",
      "significant  :  JJ\n",
      "big  :  JJ\n",
      "main  :  JJ\n",
      "key  :  JJ\n",
      "huge  :  JJ\n",
      "signficant  :  JJ\n",
      "amajor  :  JJ\n",
      "largest  :  JJS\n",
      "greatest  :  JJS\n",
      "Major  :  JJ\n",
      "massive  :  JJ\n",
      "minor  :  JJ\n",
      "substantial  :  JJ\n",
      "monumental  :  JJ\n",
      "notable  :  JJ\n",
      "signifcant  :  JJ\n",
      "big_gest  :  JJ\n",
      "MAJOR  :  NNP\n",
      "leading  :  JJ\n",
      "\n",
      "Filtered by POS (count :  16 ) ['significant', 'big', 'main', 'key', 'huge', 'signficant', 'amajor', 'Major', 'massive', 'minor', 'substantial', 'monumental', 'notable', 'signifcant', 'big_gest', 'leading']\n"
     ]
    }
   ],
   "source": [
    "synonyn_candidate_filtered = []\n",
    "\n",
    "print (thisSentence)\n",
    "for candidate in synonyn_candidate:\n",
    "    newSentence = thisSentence.replace(thisWord, candidate)\n",
    "    sentence_pos_tag = nltk.pos_tag(newSentence.split(' '))\n",
    "    print(sentence_pos_tag[word_loc][0], ' : ', sentence_pos_tag[word_loc][1])\n",
    "    if sentence_pos_tag[word_loc][1] == word_pos:\n",
    "        synonyn_candidate_filtered.append(sentence_pos_tag[word_loc][0])\n",
    "\n",
    "print('\\nFiltered by POS (count : ',len(synonyn_candidate_filtered),')',synonyn_candidate_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.14566283 -0.061473    0.8619133  ...  0.19311848 -0.06004271\n",
      "   0.12941864]]\n",
      "significant\n",
      "big\n",
      "main\n",
      "key\n",
      "huge\n",
      "signficant\n",
      "amajor\n",
      "Major\n",
      "massive\n",
      "minor\n",
      "substantial\n",
      "monumental\n",
      "notable\n",
      "signifcant\n",
      "big_gest\n",
      "leading\n",
      "[(('major', 'significant'), 0.31172402414365585), (('major', 'big'), 0.3044875228571017), (('major', 'main'), 0.2945632668917523), (('major', 'key'), 0.3255786236565861), (('major', 'huge'), 0.3680712580929485), (('major', 'signficant'), 0.35003877692299257), (('major', 'amajor'), 0.3477929437018048), (('major', 'Major'), 0.3064411874801779), (('major', 'massive'), 0.33748969969266407), (('major', 'minor'), 0.31591775857293414), (('major', 'substantial'), 0.3898059151436981), (('major', 'monumental'), 0.4440521204330301), (('major', 'notable'), 0.4499687931305507), (('major', 'signifcant'), 0.4894332179219061), (('major', 'big_gest'), 0.5712501729401156), (('major', 'leading'), 0.49074622776537846)]\n"
     ]
    }
   ],
   "source": [
    "sentence_split = thisSentence.split()\n",
    "embeddings = elmo(sentence_split, signature=\"default\", as_dict=True)[\"elmo\"]\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "main_embed = sess.run(embeddings)\n",
    "print(main_embed[word_loc])\n",
    "\n",
    "distances = []\n",
    "for idx, word in enumerate(synonyn_candidate_filtered):\n",
    "    sentence_temp = thisSentence.replace(thisWord, word)\n",
    "    sentence_split = sentence_temp.split()\n",
    "    embeddings = elmo(sentence_split, signature=\"default\", as_dict=True)[\"elmo\"]\n",
    "    \n",
    "    init = tf.initialize_all_variables()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    embed = sess.run(embeddings)\n",
    "    \n",
    "    distance = spatial.distance.cosine(main_embed[word_loc], embed[word_loc])\n",
    "    distances.append(((thisWord, word), distance))\n",
    "    \n",
    "    print(word)\n",
    "    \n",
    "print(distances)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
