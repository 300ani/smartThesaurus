{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'rnn.rnnlm_test' from '/Users/jeylee/MIDS/W266/smartThesaurus/Code/rnn/rnnlm_test.py'>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim                     # implements word2vec model infrastructure and provides interfacing APIs \n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.test.utils import get_tmpfile\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import json, os, re, shutil, sys, time\n",
    "from importlib import reload\n",
    "import collections, itertools\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "# Helper libraries\n",
    "from rnn.w266_common import utils, vocabulary, tf_embed_viz\n",
    "\n",
    "# Your code\n",
    "import rnn.rnnlm as rnnlm;reload(rnnlm)\n",
    "import rnn.rnnlm_test as rnnlm_test;reload(rnnlm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load pre-trained word2vec model\n",
    "word2vec_vectors = '../pretrained/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "w2v = gensim.models.KeyedVectors.load_word2vec_format(word2vec_vectors, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load pre-trained GloVe model\n",
    "glove_vectors = '../pretrained/glove.twitter.27B.200d.txt'\n",
    "tmp_file = get_tmpfile(\"test_word2vec.txt\")\n",
    "\n",
    "glove2word2vec(glove_input_file=glove_vectors, word2vec_output_file=tmp_file)\n",
    "glove = gensim.models.KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec cosine similarity of ['minor', 'small']: 0.3416362404823303\n",
      "word2vec cosine similarity of ['minor', 'major']: 0.47539088129997253\n",
      "\n",
      "GloVe cosine similarity of ['minor', 'small']: 0.42706066370010376\n",
      "GloVe cosine similarity of ['minor', 'major']: 0.7037895321846008\n"
     ]
    }
   ],
   "source": [
    "# similarity \n",
    "pair1 = ['minor','small']\n",
    "pair2 = ['minor','major']\n",
    "cos_dist1_w = w2v.similarity(pair1[0], pair1[1])\n",
    "cos_dist2_w = w2v.similarity(pair2[0], pair2[1])\n",
    "\n",
    "print('word2vec cosine similarity of {}: {}'.format(pair1, cos_dist1_w) )\n",
    "print('word2vec cosine similarity of {}: {}'.format(pair2, cos_dist2_w) )\n",
    "\n",
    "cos_dist1_g = glove.similarity(pair1[0], pair1[1])\n",
    "cos_dist2_g = glove.similarity(pair2[0], pair2[1])\n",
    "\n",
    "print('\\nGloVe cosine similarity of {}: {}'.format(pair1, cos_dist1_g) )\n",
    "print('GloVe cosine similarity of {}: {}'.format(pair2, cos_dist2_g) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec Vector embedding dimension:  (300,)\n",
      "\n",
      "Printing a subset of the whole vector for the word 'minor':\n",
      "[ 0.06640625 -0.00228882  0.00402832 -0.28710938 -0.21972656  0.34765625\n",
      " -0.00494385 -0.01757812  0.12988281 -0.15917969 -0.15527344 -0.16992188\n",
      "  0.06933594 -0.14257812 -0.07958984  0.16992188  0.12109375  0.125\n",
      " -0.06494141]\n",
      "\n",
      "GloVe vector embedding dimension:  (200,)\n",
      "\n",
      "Printing a subset of the whole vector for the word 'minor':\n",
      "[-0.81608999 -0.10689    -0.53272998 -0.20412    -0.37599     0.12386\n",
      " -0.12322    -0.80023998 -0.017576    0.30317    -0.068888   -1.09749997\n",
      " -0.56645     0.37650999 -0.46614999 -0.42359    -0.076921   -0.012701\n",
      " -0.0067806 ]\n"
     ]
    }
   ],
   "source": [
    "# vector representation of the word\n",
    "vec_pair1_0_w = w2v.get_vector(pair1[0])\n",
    "print(\"word2vec Vector embedding dimension: \",vec_pair1_0_w.shape)\n",
    "print(\"\\nPrinting a subset of the whole vector for the word '{}':\".format(pair1[0]))\n",
    "print(vec_pair1_0_w[1:20])\n",
    "\n",
    "vec_pair1_0_g = glove.get_vector(pair1[0])\n",
    "print(\"\\nGloVe vector embedding dimension: \",vec_pair1_0_g.shape)\n",
    "print(\"\\nPrinting a subset of the whole vector for the word '{}':\".format(pair1[0]))\n",
    "print(vec_pair1_0_g[1:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar 15 words (by word) for 'major' by word2vec model:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('biggest', 0.6572940349578857),\n",
       " ('significant', 0.619140088558197),\n",
       " ('big', 0.6057686805725098),\n",
       " ('main', 0.5380213260650635),\n",
       " ('key', 0.5354758501052856),\n",
       " ('huge', 0.5329675674438477),\n",
       " ('signficant', 0.5157025456428528),\n",
       " ('amajor', 0.49914824962615967),\n",
       " ('largest', 0.49542921781539917),\n",
       " ('greatest', 0.49444860219955444),\n",
       " ('Major', 0.4887048304080963),\n",
       " ('massive', 0.4786102771759033),\n",
       " ('minor', 0.47539088129997253),\n",
       " ('substantial', 0.46729934215545654),\n",
       " ('monumental', 0.46554118394851685)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most similar 15 words (by word) for 'major' by GloVe model:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('minor', 0.703789472579956),\n",
       " ('huge', 0.6762630939483643),\n",
       " ('massive', 0.655586838722229),\n",
       " ('big', 0.6330057382583618),\n",
       " ('biggest', 0.6215412616729736),\n",
       " ('another', 0.6144846081733704),\n",
       " ('third', 0.6137520670890808),\n",
       " ('any', 0.6084322333335876),\n",
       " ('serious', 0.6081491112709045),\n",
       " ('issues', 0.6023705005645752),\n",
       " ('first', 0.5963584780693054),\n",
       " ('having', 0.5878738164901733),\n",
       " ('two', 0.5866069197654724),\n",
       " ('other', 0.5805503129959106),\n",
       " ('many', 0.5805253982543945)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# most similar words - by word\n",
    "n_similar = 15\n",
    "thisWord = 'major'\n",
    "\n",
    "print(\"Most similar {} words (by word) for '{}' by word2vec model:\".format(n_similar, thisWord))\n",
    "display(w2v.similar_by_word(thisWord, n_similar))\n",
    "print(\"\\nMost similar {} words (by word) for '{}' by GloVe model:\".format(n_similar, thisWord))\n",
    "display(glove.similar_by_word(thisWord, n_similar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_shapes_embed (rnn.rnnlm_test.TestRNNLMCore) ... ok\n",
      "test_shapes_output (rnn.rnnlm_test.TestRNNLMCore) ... ok\n",
      "test_shapes_recurrent (rnn.rnnlm_test.TestRNNLMCore) ... ok\n",
      "test_shapes_train (rnn.rnnlm_test.TestRNNLMTrain) ... ok\n",
      "test_shapes_sample (rnn.rnnlm_test.TestRNNLMSampler) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 8.364s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "reload(rnnlm)\n",
    "\n",
    "TF_GRAPHDIR = \"/Users/jeylee/mids/W266/tmp/w266/a3_graph\"\n",
    "\n",
    "# Clear old log directory.\n",
    "shutil.rmtree(TF_GRAPHDIR, ignore_errors=True)\n",
    "\n",
    "lm = rnnlm.RNNLM(V=10000, H=200, num_layers=2)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "lm.BuildSamplerGraph()\n",
    "\n",
    "summary_writer = tf.summary.FileWriter(TF_GRAPHDIR, lm.graph)\n",
    "\n",
    "reload(rnnlm); reload(rnnlm_test)\n",
    "utils.run_tests(rnnlm_test, [\"TestRNNLMCore\", \"TestRNNLMTrain\", \"TestRNNLMSampler\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=10, learning_rate=None):\n",
    "    assert(learning_rate is not None)\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step_\n",
    "        use_dropout = True\n",
    "        loss = lm.train_loss_\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "        loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "    for i, (w, y) in enumerate(batch_iterator):\n",
    "        # At first batch in epoch, get a clean intitial state.\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "\n",
    "        feed_dict = {\n",
    "            lm.input_w_: w,\n",
    "            lm.target_y_: y,\n",
    "            lm.initial_h_: h,\n",
    "            lm.learning_rate_: learning_rate,\n",
    "            lm.use_dropout_: use_dropout\n",
    "        }\n",
    "        ops = [loss, lm.final_h_, train_op]        \n",
    "        #### YOUR CODE HERE ####\n",
    "        # session.run(...) the ops with the feed_dict constructed above.\n",
    "        # Ensure \"cost\" becomes the value of \"loss\".\n",
    "        # Hint: see \"ops\" for other variables that need updating in this loop.\n",
    "        \n",
    "        cost, h, _ = session.run([loss, lm.final_h_ ,train_op], feed_dict=feed_dict)\n",
    "        \n",
    "        #### END(YOUR CODE) ####\n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "        ##\n",
    "        # Print average loss-so-far for epoch\n",
    "        # If using train_loss_, this may be an underestimate.\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print(\"[batch {:d}]: seen {:d} words at {:.1f} wps, loss = {:.3f}\".format(\n",
    "                i, total_words, avg_wps, avg_cost))\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches\n",
    "\n",
    "def score_dataset(lm, session, ids, name=\"Data\"):\n",
    "    # For scoring, we can use larger batches to speed things up.\n",
    "    bi = utils.rnnlm_batch_generator(ids, batch_size=100, max_time=100)\n",
    "    cost = run_epoch(lm, session, bi, \n",
    "                     learning_rate=0.0, train=False, \n",
    "                     verbose=False, tick_s=3600)\n",
    "    print(\"{:s}: avg. loss: {:.03f}  (perplexity: {:.02f})\".format(name, cost, np.exp(cost)))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_toy_model (rnn.rnnlm_test.RunEpochTester) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 128]: seen 6450 words at 6391.4 wps, loss = 0.706\n",
      "[batch 275]: seen 13800 words at 6863.7 wps, loss = 0.414\n",
      "[batch 427]: seen 21400 words at 7104.2 wps, loss = 0.307\n",
      "[batch 580]: seen 29050 words at 7234.1 wps, loss = 0.249\n",
      "[batch 735]: seen 36800 words at 7336.2 wps, loss = 0.213\n",
      "[batch 890]: seen 44550 words at 7401.5 wps, loss = 0.188\n",
      "[batch 1051]: seen 52600 words at 7491.5 wps, loss = 0.170\n",
      "[batch 1204]: seen 60250 words at 7510.1 wps, loss = 0.156\n",
      "[batch 1357]: seen 67900 words at 7521.9 wps, loss = 0.145\n",
      "[batch 1518]: seen 75950 words at 7574.2 wps, loss = 0.136\n",
      "[batch 1658]: seen 82950 words at 7519.7 wps, loss = 0.129\n",
      "[batch 1777]: seen 88900 words at 7387.2 wps, loss = 0.124\n",
      "[batch 1905]: seen 95300 words at 7309.4 wps, loss = 0.119\n",
      "[batch 2023]: seen 101200 words at 7204.9 wps, loss = 0.115\n",
      "[batch 2121]: seen 106100 words at 7050.0 wps, loss = 0.112\n",
      "[batch 2266]: seen 113350 words at 7062.3 wps, loss = 0.108\n",
      "[batch 2428]: seen 121450 words at 7120.1 wps, loss = 0.105\n",
      "[batch 2587]: seen 129400 words at 7163.6 wps, loss = 0.101\n",
      "[batch 2748]: seen 137450 words at 7208.7 wps, loss = 0.098\n",
      "[batch 2905]: seen 145300 words at 7239.6 wps, loss = 0.095\n",
      "[batch 3065]: seen 153300 words at 7273.9 wps, loss = 0.093\n",
      "[batch 3227]: seen 161400 words at 7309.8 wps, loss = 0.091\n",
      "[batch 3388]: seen 169450 words at 7340.9 wps, loss = 0.089\n",
      "[batch 3545]: seen 177300 words at 7361.7 wps, loss = 0.087\n",
      "[batch 3703]: seen 185200 words at 7382.6 wps, loss = 0.085\n",
      "[batch 3863]: seen 193200 words at 7405.5 wps, loss = 0.083\n",
      "Train set: avg. loss: 0.000  (perplexity: 1.00)\n",
      "Test set: avg. loss: 0.000  (perplexity: 1.00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 30.042s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(rnnlm); reload(rnnlm_test)\n",
    "th = rnnlm_test.RunEpochTester(\"test_toy_model\")\n",
    "th.setUp(); th.injectCode(run_epoch, score_dataset)\n",
    "unittest.TextTestRunner(verbosity=2).run(th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/jeylee/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "Vocabulary: 10,000 types\n",
      "Loaded 57,340 sentences (1.16119e+06 tokens)\n",
      "Training set: 45,872 sentences (924,077 tokens)\n",
      "Test set: 11,468 sentences (237,115 tokens)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "V = 10000\n",
    "vocab, train_ids, test_ids = utils.load_corpus(\"brown\", split=0.8, V=V, shuffle=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "max_time = 25\n",
    "batch_size = 100\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(V=vocab.size, \n",
    "                    H=200, \n",
    "                    softmax_ns=200,\n",
    "                    num_layers=2)\n",
    "\n",
    "TF_SAVEDIR = \"/Users/jeylee/mids/W266/tmp/w266/a3_model\"\n",
    "checkpoint_filename = os.path.join(TF_SAVEDIR, \"rnnlm\")\n",
    "trained_filename = os.path.join(TF_SAVEDIR, \"rnnlm_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] Starting epoch 1\n",
      "[epoch 1] Completed in 0:01:30\n",
      "[epoch 1] Train set: avg. loss: 5.396  (perplexity: 220.55)\n",
      "[epoch 1] Test set: avg. loss: 5.437  (perplexity: 229.64)\n",
      "\n",
      "[epoch 2] Starting epoch 2\n",
      "[epoch 2] Completed in 0:01:29\n",
      "[epoch 2] Train set: avg. loss: 5.210  (perplexity: 183.05)\n",
      "[epoch 2] Test set: avg. loss: 5.267  (perplexity: 193.86)\n",
      "\n",
      "[epoch 3] Starting epoch 3\n",
      "[epoch 3] Completed in 0:01:31\n",
      "[epoch 3] Train set: avg. loss: 5.118  (perplexity: 166.93)\n",
      "[epoch 3] Test set: avg. loss: 5.186  (perplexity: 178.80)\n",
      "\n",
      "[epoch 4] Starting epoch 4\n",
      "[epoch 4] Completed in 0:01:32\n",
      "[epoch 4] Train set: avg. loss: 5.032  (perplexity: 153.30)\n",
      "[epoch 4] Test set: avg. loss: 5.115  (perplexity: 166.49)\n",
      "\n",
      "[epoch 5] Starting epoch 5\n",
      "[epoch 5] Completed in 0:01:32\n",
      "[epoch 5] Train set: avg. loss: 4.972  (perplexity: 144.37)\n",
      "[epoch 5] Test set: avg. loss: 5.066  (perplexity: 158.53)\n",
      "\n",
      "[epoch 6] Starting epoch 6\n",
      "[epoch 6] Completed in 0:01:33\n",
      "[epoch 6] Train set: avg. loss: 4.921  (perplexity: 137.09)\n",
      "[epoch 6] Test set: avg. loss: 5.026  (perplexity: 152.32)\n",
      "\n",
      "[epoch 7] Starting epoch 7\n",
      "[epoch 7] Completed in 0:01:32\n",
      "[epoch 7] Train set: avg. loss: 4.882  (perplexity: 131.93)\n",
      "[epoch 7] Test set: avg. loss: 4.999  (perplexity: 148.23)\n",
      "\n",
      "[epoch 8] Starting epoch 8\n",
      "[epoch 8] Completed in 0:01:49\n",
      "[epoch 8] Train set: avg. loss: 4.858  (perplexity: 128.81)\n",
      "[epoch 8] Test set: avg. loss: 4.983  (perplexity: 145.95)\n",
      "\n",
      "[epoch 9] Starting epoch 9\n",
      "[epoch 9] Completed in 0:01:47\n",
      "[epoch 9] Train set: avg. loss: 4.833  (perplexity: 125.53)\n",
      "[epoch 9] Test set: avg. loss: 4.968  (perplexity: 143.74)\n",
      "\n",
      "[epoch 10] Starting epoch 10\n",
      "[epoch 10] Completed in 0:01:56\n",
      "[epoch 10] Train set: avg. loss: 4.805  (perplexity: 122.11)\n",
      "[epoch 10] Test set: avg. loss: 4.951  (perplexity: 141.28)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Will print status every this many seconds\n",
    "print_interval = 5\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "\n",
    "# Explicitly add global initializer and variable saver to LM graph\n",
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "if not os.path.isdir(TF_SAVEDIR):\n",
    "    os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    session.run(initializer)\n",
    "\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        bi = utils.rnnlm_batch_generator(train_ids, batch_size, max_time)\n",
    "        print(\"[epoch {:d}] Starting epoch {:d}\".format(epoch, epoch))\n",
    "        #### YOUR CODE HERE ####\n",
    "        # Run a training epoch.\n",
    "        \n",
    "        run_epoch(lm, session, bi, learning_rate=learning_rate, train=True, verbose=False, tick_s=10)\n",
    "        \n",
    "        #### END(YOUR CODE) ####\n",
    "        print(\"[epoch {:d}] Completed in {:s}\".format(epoch, utils.pretty_timedelta(since=t0_epoch)))\n",
    "    \n",
    "        # Save a checkpoint\n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "    \n",
    "        ##\n",
    "        # score_dataset will run a forward pass over the entire dataset\n",
    "        # and report perplexity scores. This can be slow (around 1/2 to \n",
    "        # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "        # to speed up training on a slow machine. Be sure to run it at the \n",
    "        # end to evaluate your score.\n",
    "        print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "        score_dataset(lm, session, train_ids, name=\"Train set\")\n",
    "        print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "        score_dataset(lm, session, test_ids, name=\"Test set\")\n",
    "        print(\"\")\n",
    "    \n",
    "    # Save final model\n",
    "    saver.save(session, trained_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_seq(lm, session, seq, vocab):\n",
    "    \"\"\"Score a sequence of words. Returns total log-probability.\"\"\"\n",
    "    padded_ids = vocab.words_to_ids(utils.canonicalize_words([\"<s>\"] + seq + [\"</s>\"], \n",
    "                                                             wordset=vocab.word_to_id))\n",
    "    w = np.reshape(padded_ids[:-1], [1,-1])\n",
    "    y = np.reshape(padded_ids[1:],  [1,-1])\n",
    "    h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "    feed_dict = {lm.input_w_:w,\n",
    "                 lm.target_y_:y,\n",
    "                 lm.initial_h_:h,\n",
    "                 lm.dropout_keep_prob_: 1.0}\n",
    "    # Return log(P(seq)) = -1*loss\n",
    "    return -1*session.run(lm.loss_, feed_dict)\n",
    "\n",
    "def load_and_score(inputs, sort=True):\n",
    "    \"\"\"Load the trained model and score the given words.\"\"\"\n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    \n",
    "    with lm.graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session(graph=lm.graph) as session:  \n",
    "        # Load the trained model\n",
    "        saver.restore(session, trained_filename)\n",
    "\n",
    "        if isinstance(inputs[0], str) or isinstance(inputs[0], bytes):\n",
    "            inputs = [inputs]\n",
    "\n",
    "        # Actually run scoring\n",
    "        results = []\n",
    "        for words in inputs:\n",
    "            score = score_seq(lm, session, words, vocab)\n",
    "            sentence_gen = \" \".join(words)\n",
    "            results.append((score, sentence_gen))\n",
    "\n",
    "        # Sort if requested\n",
    "        if sort: results = sorted(results, reverse=True)\n",
    "            \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/jeylee/mids/W266/tmp/w266/a3_model/rnnlm_trained\n",
      "[(-6.7438759803771973, 'No one but the Creator understands their internal logic.'), (-8.6257171630859375, 'I love you')]\n"
     ]
    }
   ],
   "source": [
    "sents = [\"No one but the Creator understands their internal logic.\",\"I love you\"]\n",
    "result = load_and_score([s.split() for s in sents])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar 15 words (by vector) for 'major' by word2vec model:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('biggest', 0.6572940349578857),\n",
       " ('significant', 0.619140088558197),\n",
       " ('big', 0.6057686805725098),\n",
       " ('main', 0.5380213260650635),\n",
       " ('key', 0.5354758501052856),\n",
       " ('huge', 0.5329675674438477),\n",
       " ('signficant', 0.5157025456428528),\n",
       " ('amajor', 0.49914824962615967),\n",
       " ('largest', 0.49542921781539917),\n",
       " ('greatest', 0.49444860219955444),\n",
       " ('Major', 0.4887048304080963),\n",
       " ('massive', 0.4786102771759033),\n",
       " ('minor', 0.47539088129997253),\n",
       " ('substantial', 0.46729934215545654),\n",
       " ('monumental', 0.46554118394851685)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most similar 15 words (by vector) for 'major' by GloVe model:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('minor', 0.703789472579956),\n",
       " ('huge', 0.6762630939483643),\n",
       " ('massive', 0.655586838722229),\n",
       " ('big', 0.6330057382583618),\n",
       " ('biggest', 0.6215412616729736),\n",
       " ('another', 0.6144846081733704),\n",
       " ('third', 0.6137520670890808),\n",
       " ('any', 0.6084322333335876),\n",
       " ('serious', 0.6081491112709045),\n",
       " ('issues', 0.6023705005645752),\n",
       " ('first', 0.5963584780693054),\n",
       " ('having', 0.5878738164901733),\n",
       " ('two', 0.5866069197654724),\n",
       " ('other', 0.5805503129959106),\n",
       " ('many', 0.5805253982543945)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# most similar words - by vector\n",
    "print(\"Most similar {} words (by vector) for '{}' by word2vec model:\".format(n_similar, thisWord))\n",
    "display(w2v.similar_by_vector(thisWord, n_similar))\n",
    "print(\"\\nMost similar {} words (by vector) for '{}' by GloVe model:\".format(n_similar, thisWord))\n",
    "display(glove.similar_by_vector(thisWord, n_similar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Most similar 15 words (by vector) for 'author' by word2vec model:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('coauthor', 0.7295109033584595),\n",
       " ('Author', 0.7111563682556152),\n",
       " ('authors', 0.6822032332420349),\n",
       " ('bestselling_author', 0.6656025052070618),\n",
       " ('article_Anatoly_Yablokov', 0.6428576707839966),\n",
       " ('co_authored', 0.642702043056488),\n",
       " ('coauthored', 0.6324652433395386),\n",
       " ('novelist', 0.6296602487564087),\n",
       " (\"Hazel_Lodevico_To'o_Email\", 0.6230490207672119),\n",
       " ('Giuseppe_Ungaro_Email', 0.614190936088562),\n",
       " ('authored', 0.6116317510604858),\n",
       " ('Laura_Silvius_Email', 0.6090387105941772),\n",
       " ('Sheila_Naturinda_email', 0.609026312828064),\n",
       " ('Allison_Eatough_Email', 0.6082030534744263),\n",
       " ('Ariane_Sommer', 0.6075382232666016)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Most similar 15 words (by vector) for 'author' by GloVe model:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('writer', 0.7710938453674316),\n",
       " ('authors', 0.7271348237991333),\n",
       " ('book', 0.6985723376274109),\n",
       " ('books', 0.6393362879753113),\n",
       " ('paperback', 0.6384525299072266),\n",
       " ('novelist', 0.6199874877929688),\n",
       " ('ebook', 0.6190778613090515),\n",
       " ('fiction', 0.6185603141784668),\n",
       " ('excerpt', 0.6143531203269958),\n",
       " ('publisher', 0.611627995967865),\n",
       " ('writers', 0.6092760562896729),\n",
       " ('kindle', 0.6059399843215942),\n",
       " ('blogger', 0.5961562395095825),\n",
       " ('poet', 0.5884875059127808),\n",
       " ('novel', 0.5830292701721191)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Most similar 15 words for 'author' by word2vec + RNN model:\n",
      "\n",
      "The coauthored shows promise of better things  :  -7.29312229156\n",
      "The coauthor shows promise of better things  :  -7.29312229156\n",
      "The co_authored shows promise of better things  :  -7.29312229156\n",
      "The bestselling_author shows promise of better things  :  -7.29312229156\n",
      "The authored shows promise of better things  :  -7.29312229156\n",
      "The article_Anatoly_Yablokov shows promise of better things  :  -7.29312229156\n",
      "The Sheila_Naturinda_email shows promise of better things  :  -7.29312229156\n",
      "The Laura_Silvius_Email shows promise of better things  :  -7.29312229156\n",
      "The Hazel_Lodevico_To'o_Email shows promise of better things  :  -7.29312229156\n",
      "The Giuseppe_Ungaro_Email shows promise of better things  :  -7.29312229156\n",
      "The Ariane_Sommer shows promise of better things  :  -7.29312229156\n",
      "The Allison_Eatough_Email shows promise of better things  :  -7.29312229156\n",
      "The novelist shows promise of better things  :  -8.15892601013\n",
      "The Author shows promise of better things  :  -8.15933609009\n",
      "The authors shows promise of better things  :  -8.23500156403\n",
      "\n",
      "### Most similar 15 words for 'author' by GloVe + RNN model:\n",
      "\n",
      "The paperback shows promise of better things  :  -7.29312229156\n",
      "The kindle shows promise of better things  :  -7.29312229156\n",
      "The excerpt shows promise of better things  :  -7.29312229156\n",
      "The ebook shows promise of better things  :  -7.29312229156\n",
      "The blogger shows promise of better things  :  -7.29312229156\n",
      "The writer shows promise of better things  :  -7.89785575867\n",
      "The book shows promise of better things  :  -7.95880460739\n",
      "The novel shows promise of better things  :  -8.08187007904\n",
      "The poet shows promise of better things  :  -8.09954833984\n",
      "The novelist shows promise of better things  :  -8.15892601013\n",
      "The books shows promise of better things  :  -8.16882896423\n",
      "The writers shows promise of better things  :  -8.22988700867\n",
      "The authors shows promise of better things  :  -8.23500156403\n",
      "The fiction shows promise of better things  :  -8.49521827698\n",
      "The publisher shows promise of better things  :  -8.5802192688\n"
     ]
    }
   ],
   "source": [
    "targetSentence = 'The author shows promise of better things'\n",
    "targetWord = 'author'\n",
    "n_candidate = 15\n",
    "\n",
    "print(\"### Most similar {} words (by vector) for '{}' by word2vec model:\".format(n_candidate, targetWord))\n",
    "display(w2v.similar_by_vector(targetWord, n_candidate))\n",
    "print(\"\\n### Most similar {} words (by vector) for '{}' by GloVe model:\".format(n_candidate, targetWord))\n",
    "display(glove.similar_by_vector(targetWord, n_candidate))\n",
    "\n",
    "word2vec_candidates = w2v.similar_by_vector(targetWord, n_candidate)\n",
    "glove_candidates = glove.similar_by_vector(targetWord, n_candidate)\n",
    "\n",
    "print(\"\\n### Most similar {} words for '{}' by word2vec + RNN model:\\n\".format(n_candidate, targetWord))\n",
    "sentences_word2vec = []\n",
    "for i in word2vec_candidates:\n",
    "    sentences_word2vec.append(targetSentence.replace(targetWord, i[0]))\n",
    "word2vec_rnn_candidates = load_and_score([s.split() for s in sentences_word2vec], sort=True)\n",
    "for i in word2vec_rnn_candidates:\n",
    "    print (i[1],\" : \", i[0])\n",
    "\n",
    "print(\"\\n### Most similar {} words for '{}' by GloVe + RNN model:\\n\".format(n_candidate, targetWord))\n",
    "sentences_glove = []\n",
    "for i in glove_candidates:\n",
    "    sentences_glove.append(targetSentence.replace(targetWord, i[0]))\n",
    "glove_rnn_candidates = load_and_score([s.split() for s in sentences_glove], sort=True)\n",
    "for i in glove_rnn_candidates:\n",
    "    print (i[1],\" : \", i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simlex_synonyms = [['child','a man as simple as a child','youngster'],\n",
    "['author','The author shows promise of better things','writer'],\n",
    "['book','a freak copy of a book','work'],\n",
    "['dictionary','The dictionary is interleaved with a sheet of blank paper','wordbook'],\n",
    "['champion','Last years champion gained the lead in the race and won it','winner'],\n",
    "['air','How many times a day should we air our ad','wind'],\n",
    "['victory','The spiritual strength is just the motive power of victory','win'],\n",
    "['bath','Do you actually share the bath with other people','wash'],\n",
    "['roam','A user of the selected mail system may not roam','wander'],\n",
    "['crucial','The speed of the internet is crucial in your job search','vital'],\n",
    "['essential','Sleep and good food are essential to health','vital'],\n",
    "['winner','I do not know who the winner was','victor'],\n",
    "['car','I am looking for a used car','vehicle'],\n",
    "['disappear','Memories do not disappear They lap over each other','vanish'],\n",
    "['sick','He stayed and helped his sick friend all night','unwell'],\n",
    "['exotic','The tree right in front of us looks exotic','unusual'],\n",
    "['fragile','She looks soft but she is not fragile','unstable'],\n",
    "['restless','He was strangely restless at that time','unsettled'],\n",
    "['illegal','Do not ask him to do something illegal','unlawful'],\n",
    "['stupid','It would be really stupid for me to do this','unintelligent'],\n",
    "['sad','Do not be sad Life is like that','unhappy'],\n",
    "['rough','A car is bouncing along the rough road','uneven'],\n",
    "['wisdom','After 18 years of age wisdom teeth begin to emerge','understanding'],\n",
    "['comprehend','They did not comprehend the significance of his remark','understand'],\n",
    "['student','One student split on his friend','undergraduate'],\n",
    "['simple','a man as simple as a child','uncomplicated'],\n",
    "['pipe','I do not know how to dance to her pipe','tube'],\n",
    "['fact','As a matter of fact my mother wants to sell her car','truth'],\n",
    "['honest','I judge him to be a very honest man','trustworthy'],\n",
    "['belief','I was confirmed in my belief with the lapse of time','trust'],\n",
    "['confidence','Self confidence is the most important key to success','trust'],\n",
    "['succeed','I hope I shall succeed this time','triumph'],\n",
    "['journey','The day when I start for my journey is drawing near','trip'],\n",
    "['business','He works as a business consultant','trade'],\n",
    "['hard','I am hard at work on my next presentation','tough'],\n",
    "['finger','People pointed the finger at the mayor','touch'],\n",
    "['date','What is the date today over there','time'],\n",
    "['wood','A drill is lying on top of some wood','timber'],\n",
    "['attention','A loud noise diverted everyones attention from their work','thinking'],\n",
    "['narrow','No one accepted his narrow minded political views','thin'],\n",
    "['word','Word has it that they are not together anymore','term'],\n",
    "['muscle','They tend to lose some muscle mass every year','tendon'],\n",
    "['storm','A storm hit the country and killed a lot of people','tempest'],\n",
    "['inform','Why did not you inform me of this','tell'],\n",
    "['cab','Most people often have to share a cab with other passengers','taxi'],\n",
    "['assignment','You must hand in your assignment on time','task'],\n",
    "['discussion','After a long discussion they finally could abide the issue','talk'],\n",
    "['speak','The dog is so to speak a member of the family','talk'],\n",
    "['story','Let me hear the story or Tell me the story','tale'],\n",
    "['steal','Can I steal a minute of your time','take'],\n",
    "['nail','I want to nail it down','tack'],\n",
    "['lover','a lover of books','sweetheart'],\n",
    "['certain','You can be certain I will never do it again','sure'],\n",
    "['agony','The man cries in great agony','suffering'],\n",
    "['pain','She was in a lot of pain','suffering'],\n",
    "['log','People are sitting there like bumps on a log','stump'],\n",
    "['pupil','Each pupil has his own desk','student'],\n",
    "['anatomy','Define the anatomy to be imaged','structure'],\n",
    "['river','The man is fishing by the river','stream'],\n",
    "['bizarre','She died under bizarre circumstances no one knows how','strange'],\n",
    "['weird','He is charming and weird at the same time','strange'],\n",
    "['belly','I do not want a beer belly like my father','stomach'],\n",
    "['rod','This steel rod is straight as a back leg of a dog','stick'],\n",
    "['root','Lets eliminate the problem root and branch','stem'],\n",
    "['remain','Only the really bad people will decide to remain criminals','stay'],\n",
    "['politician','The politician nixed out one day','statesman'],\n",
    "['condition','the circumstances which condition our lives','state'],\n",
    "['declare','I wish to declare that I am certain of success','state'],\n",
    "['say','I suppose but they say it is very expensive','state'],\n",
    "['begin','After 18 years of age wisdom teeth begin to emerge','start'],\n",
    "['groom','The father is handing her to the groom','stableman'],\n",
    "['wife','Bring your wife and children too','spouse'],\n",
    "['ball','You can hear a description of the ball game on the radio','sphere'],\n",
    "['wide','A wide prospect burst upon my view','spacious'],\n",
    "['noise','The elevator stopped working with a loud noise','sound'],\n",
    "['army','the effective strength of an army','soldiers'],\n",
    "['nose','His guilt is as plain as the nose on his face','snout'],\n",
    "['tiny','Tiny plants float on the water and are a food source','small'],\n",
    "['kill','I will kill him five times before he hits the ground','slay'],\n",
    "['head','I worked in the head office of Bloomberg for seven years','skull'],\n",
    "['circumstance','They are friends that treat each other without circumstance','situation'],\n",
    "['easy','No we are still working on it It is not an easy task','simple'],\n",
    "['illness','His absence from work was because of his illness','sickness'],\n",
    "['beach','The man and woman are walking on a beach','shore'],\n",
    "['vessel','This vessel holds a lot of water','ship'],\n",
    "['harsh','She was harsh to her children','severe'],\n",
    "['formal','Use a colon after the salutation of a formal letter','serious'],\n",
    "['appointment','What time is my next appointment','selection'],\n",
    "['apparent','It is apparent that he wrote the letter himself','seeming'],\n",
    "['chapter','the final chapter of a book','section'],\n",
    "['afraid','The man is afraid of the mouse','scared'],\n",
    "['read','I usually read books but I watch TV at other times','scan'],\n",
    "['moon','Is it the sun or the moon','satellite'],\n",
    "['greet','She did not want to greet him so she looked the other way','salute'],\n",
    "['unhappy','He thinks the people will be unhappy in the future','sad'],\n",
    "['king','The King holds dominion over the people of his nation','ruler'],\n",
    "['prince','The crown prince is in training for becoming king one day','ruler'],\n",
    "['princess','Now be happy as a beautiful princess night and day','ruler'],\n",
    "['destroy','You want to destroy yourself you do it on your own','ruin'],\n",
    "['destruction','The pomps and vanities will bring the destruction','ruin'],\n",
    "['course','She led a one week course last year','route'],\n",
    "['cabin','Emergency exits are located on both sides of the cabin','room'],\n",
    "['sly','His lips were spread in a sly smile','roguish'],\n",
    "['street','The main street is busy all the times of the year','road'],\n",
    "['wealth','There are a lot of people who gained sudden wealth','riches'],\n",
    "['save','Save it for a rainy day','rescue'],\n",
    "['withdraw','They threatened to withdraw from the talks','remove'],\n",
    "['area','The farmers in this area are very busy at this time of year','region'],\n",
    "['advise','I advise you to leave the student problems severely alone','recommend'],\n",
    "['accept','Kindly accept a copy of my work just out','receive'],\n",
    "['reject','This scheme is so diabolical that I must reject it','rebuff'],\n",
    "['argument','He carefully dealt with a very sensitive argument','reason'],\n",
    "['logic','There is a jump in the logic of his opinion','reason'],\n",
    "['accomplish','I would accomplish so much more that way','realize'],\n",
    "['choice','The college offers a wide choice of courses','range'],\n",
    "['rain','Rain in Christmas time is visit of an angel','rainfall'],\n",
    "['anger','She drives like Jeho in a fit of anger','rage'],\n",
    "['fast','Driving too fast puts people in danger','quick'],\n",
    "['argue','I argue with my brother all the time','quarrel'],\n",
    "['heroine','Her reputation as a heroine grew and grew','protagonist'],\n",
    "['make','make a thing a matter of conscience','produce'],\n",
    "['jail','After all he landed up in jail','prison'],\n",
    "['code','Please reenter your personal code number','principles'],\n",
    "['leader','The Khmer Rouge leader Pol Pot died eight years ago','principal'],\n",
    "['keep','He wanted to keep out of it','prevent'],\n",
    "['assume','The matter threatens to assume serious proportions','presume'],\n",
    "['bias','Cultural bias has many problems that must be solved','prejudice'],\n",
    "['strong','Competition for such jobs will be strong','powerful'],\n",
    "['purse','Oh no I think I left my purse behind','pouch'],\n",
    "['jar','Collect change in a jar for a whole month','pot'],\n",
    "['job','We buy our supplies in job lots','position'],\n",
    "['situation','It looks like you are in a very tough situation','position'],\n",
    "['lake','measure the magnitude of a lake','pond'],\n",
    "['abundance','North America is a land of abundance','plenty'],\n",
    "['happy','No family can be happy without harmony among its members','pleased'],\n",
    "['delightful','a delightful situation for a house','pleasant'],\n",
    "['nice','It is nice but I do not want one','pleasant'],\n",
    "['portray','Ueno will portray a substitute high school teacher','play'],\n",
    "['plate','The woman is putting food on her plate','platter'],\n",
    "['arrange','Did you arrange those circus tickets for next week','plan'],\n",
    "['put','It is time to put a period to the matter','place'],\n",
    "['choose','I had to choose only one person among those people','pick'],\n",
    "['body','the economy of the human body','physique'],\n",
    "['doctor','There is a doctor on call 24 hours a day','physician'],\n",
    "['telephone','A person calls you on the telephone','phone'],\n",
    "['people','the numerous voice of the people','persons'],\n",
    "['actor','The actor is on the bill','performer'],\n",
    "['do','When do you show the movie','perform'],\n",
    "['give','give a person a fair hearing','perform'],\n",
    "['mountain','The people are on top of a mountain','peak'],\n",
    "['top','The people are on top of a mountain','peak'],\n",
    "['gut','I thought I would bust a gut laughing','paunch'],\n",
    "['game','So exciting to watch the soccer game with my friends','pastime'],\n",
    "['aisle','A man and beautiful woman walked down the aisle','passageway'],\n",
    "['alley','The troops passed into their alley','passage'],\n",
    "['celebration','I think this calls for a celebration','party'],\n",
    "['wall','The car dashed into a wall','partition'],\n",
    "['atom','He has not an ounce an atom of conscience','particle'],\n",
    "['molecule','They join together into one giant molecule','particle'],\n",
    "['limb','He lost his leg in an accident and wears an artificial limb','part'],\n",
    "['hand','The gentleman offered his hand to the lady','palm'],\n",
    "['couple','He has been with this company only for a couple of years','pair'],\n",
    "['adversary','A boxer sprang at his adversary','opponent'],\n",
    "['attitude','I perceived a slight change in his attitude','opinion'],\n",
    "['door','I felt my way to the door','opening'],\n",
    "['crime','He was caught as a party to the crime','offence'],\n",
    "['strange','Fate works in a strange way','odd'],\n",
    "['sea','I worked my way up along the East Sea from there','ocean'],\n",
    "['noticeable','This threat had a noticeable effect on the crowd','obvious'],\n",
    "['idea','print an idea on the mind','notion'],\n",
    "['north','a house with a north aspect','northern'],\n",
    "['racket','Hold the racket so like this','noise'],\n",
    "['sunset','The view of the sunset from here is out of sight','nightfall'],\n",
    "['paper','There is no use in trying to paper over your faults','newspaper'],\n",
    "['recent','His novels have gained in popularity over recent years','new'],\n",
    "['forget','Do not forget that I am always by you','neglect'],\n",
    "['denial','My father is in serious denial about it','negation'],\n",
    "['necessary','Oh I did not know that was necessary','needed'],\n",
    "['require','Highland games only require sports skills','need'],\n",
    "['horse','The match was horse and horse','nag'],\n",
    "['crowd','He pushed his way through a crowd','multitude'],\n",
    "['cup','The World Cup amused the people across the country','mug'],\n",
    "['go','That hat doesn not go very well with the dress','move'],\n",
    "['hill','There is a station right over the hill','mount'],\n",
    "['second','This reader is too difficult for second year grade pupils','moment'],\n",
    "['new','The Chinese New Year is a 15 day holiday','modern'],\n",
    "['modest','You really should not be so modest all the time','moderate'],\n",
    "['cloud','a cloud of war','mist'],\n",
    "['strength','the effective strength of an army','might'],\n",
    "['formula','“Very truly yours” is a formula used in letters','method'],\n",
    "['way','We need it in a kind of way','method'],\n",
    "['letter','How many days of the week start with the letter T','message'],\n",
    "['deserve','He doee not deserve to be told off','merit'],\n",
    "['dinner','Serving breakfast lunch and dinner seven days a week','meal'],\n",
    "['learn','The important thing is to learn from my mistakes','master'],\n",
    "['target','Please enter the distinguished name of the target OU','mark'],\n",
    "['polite','He is generous and you know very polite','mannerly'],\n",
    "['style','You can customize the style and wording of the product name','manner'],\n",
    "['administration','There is a thorough change in the administration','management'],\n",
    "['guy','Do not trust that guy man I think he is a narc','man'],\n",
    "['man','a man as simple as a child','male'],\n",
    "['creator','No one but the Creator understands their internal logic','maker'],\n",
    "['insane','His family sent their insane relative to live in a madhouse','mad'],\n",
    "['oil','Oil prices reached their peak last year','lubricate'],\n",
    "['laden','Osama bin Laden was the world most wanted terrorist','loaded'],\n",
    "['water','A man urged on people the need of water','liquid'],\n",
    "['mouth','My mouth felt as dry as a bone or My mouth felt parched','lips'],\n",
    "['leg','That is a stiff price or It costs me an arm and a leg','limb'],\n",
    "['blood','I need to let blood to test your immune system','lifeblood'],\n",
    "['reduce','The new technique will reduce the cost of production','lessen'],\n",
    "['captain','He was promoted to captain because he worked fine','leader'],\n",
    "['anarchy','At that time all was anarchy in China','lawlessness'],\n",
    "['big','The boy from the country looked gawky in the big city','large'],\n",
    "['great','His visit of state was a great event','large'],\n",
    "['tongue','The boy stuck his tongue in his cheek before his friends','language'],\n",
    "['woman','The woman is looking up to her friend','lady'],\n",
    "['boy','The man and boy are tuning the guitar','lad'],\n",
    "['sharp','The streets are very dangerous Look sharp','keen'],\n",
    "['trick','I will never miss a trick','joke'],\n",
    "['elbow','He busted up his elbow playing tennis','joint'],\n",
    "['task','No We are still working on it It is not an easy task','job'],\n",
    "['danger','He are in danger of losing his family members','jeopardy'],\n",
    "['island','He was landed on a lonely island','isle'],\n",
    "['intelligence','credit a person with rare intelligence','intellect'],\n",
    "['teacher','Do not talk back to your teacher','instructor'],\n",
    "['encourage','Bush would encourage added supplies in many ways','inspire'],\n",
    "['mad','It was a mad house in my office today','insane'],\n",
    "['ask','I wanted to ask you something','inquire'],\n",
    "['population','The population in this neighborhood increases day by day','inhabitants'],\n",
    "['multiply','Some bacteria multiply by cell division','increase'],\n",
    "['cheek','The boy stuck his tongue in his cheek before his friends','impudence'],\n",
    "['value','People do not know the value of health till they lose it','importance'],\n",
    "['beg','Do not be like this please I beg you','implore'],\n",
    "['reflection','The girl was lost in reflection','image'],\n",
    "['vision','He is a man of broad vision','image'],\n",
    "['disease','He is way ahead of me in disease research','illness'],\n",
    "['dog','A wolf is a member of the dog family','hound'],\n",
    "['aggression','I was able to hit with precision and aggression today','hostility'],\n",
    "['house','The school house is spick and span inside and out','home'],\n",
    "['container','He is pouring water from the container','holder'],\n",
    "['sky','The storm blew the house sky high','heavens'],\n",
    "['trial','It is just a process of trial and error','hearing'],\n",
    "['listen','You know what happens when you do not listen','hear'],\n",
    "['chief','the chief attraction of the day','head'],\n",
    "['bad','Be careful not to use bad language to people','harmful'],\n",
    "['difficult','This reader is too difficult for second year grade pupils','hard'],\n",
    "['hound','Defend justice as a hound of law','harass'],\n",
    "['occur','Since the Earth is rotating two tides occur each day','happen'],\n",
    "['cancer','The US death rate from cancer increased last year','growth'],\n",
    "['floor','Tony spilled the water on the floor','ground'],\n",
    "['take','You should not take too much exercise','grip'],\n",
    "['sorrow','Time blunts the edge of sorrow','grief'],\n",
    "['seed','After the seed fell on good soil it yields plenty of fruit','grain'],\n",
    "['monster','Another game shows a monster eating a person','giant'],\n",
    "['acquire','I hope the rumor does not acquire currency','get'],\n",
    "['receive','Others do not receive wages for several months at a time','get'],\n",
    "['lady','The gentleman offered his hand to the lady','gentlewoman'],\n",
    "['collect','Collect change in a jar for a whole month','gather'],\n",
    "['boundary','The neighbors had a violent dispute on the boundary','frontier'],\n",
    "['hysteria','Wherever he appeared he excited hysteria','frenzy'],\n",
    "['bread','This food is the best thing since sliced bread','food'],\n",
    "['diet','Using diet pills is not a safe way to lose weight','food'],\n",
    "['meat','The price of meat is high','food'],\n",
    "['page','Please turn to page 5 in your copy of the report','folio'],\n",
    "['elastic','When stretched a rubber band produces an elastic force','flexible'],\n",
    "['navy','The US Navy played a vital role in World War II','fleet'],\n",
    "['apartment','My home is an apartment in Manhattan','flat'],\n",
    "['gun','Shortly afterward a burst of machine gun fire was heard','firearm'],\n",
    "['locate','I can locate the lost car','find'],\n",
    "['dirty','Dirty oil can hurt an engine','filthy'],\n",
    "['movie','When do you show the movie','film'],\n",
    "['battle','The first blow is half the battle','fight'],\n",
    "['bring','Bring your wife and children too','fetch'],\n",
    "['cat','The cat slept for two hours','feline'],\n",
    "['pretend','I do not like people who pretend to be altruistic','feign'],\n",
    "['emotion','Music is a tool to express emotion','feeling'],\n",
    "['article','Newspaper reported correction article under fire','feature'],\n",
    "['bold','If I may be so bold as to say','fearless'],\n",
    "['quick','I grabbed a quick bite for lunch','fast'],\n",
    "['sense','The music fits the sense of the words like a glove','faculty'],\n",
    "['reality','The cooperative is a working reality','fact'],\n",
    "['eye','It is Friday and time for Eye on Hollywood','eyeball'],\n",
    "['phrase','The journalist turned a phrase for the journal','expression'],\n",
    "['milk','He drank a glass of milk','exploit'],\n",
    "['clarify','Identify clarify and extend Explicit Needs','explain'],\n",
    "['forgive','I will forgive you this time but do not let it happen again','excuse'],\n",
    "['fever','I need something to bring down the fever','excitement'],\n",
    "['terrific','Ladies and gentleman we have a terrific sale today','excellent'],\n",
    "['wonderful','Well whomever I am just glad I had such a wonderful meal','excellent'],\n",
    "['inspect','Do not expect what you do not inspect','examine'],\n",
    "['investigate','Mr Lagos says that his government may also investigate','examine'],\n",
    "['proof','People say the proof of the pudding is in the eating','evidence'],\n",
    "['band','It is a very family kind of band','ensemble'],\n",
    "['huge','I need you to do me a huge favor','enormous'],\n",
    "['worker','The worker is using a shovel','employee'],\n",
    "['heart','I love Tom in my heart','emotions'],\n",
    "['long','over a long term of years','elongated'],\n",
    "['comfort','People like to live in comfort','ease'],\n",
    "['world','The World Cup amused the people across the country','earth'],\n",
    "['death','What was the profession of him at the time of his death','dying'],\n",
    "['home','He beat his brother home from school','dwelling'],\n",
    "['evening','If we leave Friday evening we can come back Sunday evening','dusk'],\n",
    "['dreary','It was a dreary day cold and without sunshine','dull'],\n",
    "['physician','Social worker Physician Other service provider','doctor'],\n",
    "['different','The velocity of sound in air and water is different','dissimilar'],\n",
    "['send','How many copies do we need to send','dispatch'],\n",
    "['find','We find most of our people through employment agencies','discover'],\n",
    "['science','I felt interested in studying science','discipline'],\n",
    "['vanish','Should I just vanish to some unknown place','disappear'],\n",
    "['mud','Your coat is covered with mud','dirt'],\n",
    "['size','There are a lot of small apples of about the same size','dimensions'],\n",
    "['despair','drive a person to despair','despondency'],\n",
    "['joy','It was the time of year for joy','delight'],\n",
    "['please','O you may do as you please','delight'],\n",
    "['god','Do you ever pray to God','deity'],\n",
    "['overcome','I think we have to overcome the pollution problem first','defeat'],\n",
    "['depth','It is a question beyond my depth','deepness'],\n",
    "['action','Please refrain your action without system','deed'],\n",
    "['shrink','I do not shrink from this responsibility','decrease'],\n",
    "['deck','There was a dog below deck','decorate'],\n",
    "['conclude','Did they conclude upon an arrangement with each other','decide'],\n",
    "['night','I have night duty several times a month','darkness'],\n",
    "['father','My father reached the age of sixty this year','daddy'],\n",
    "['loop','The news blows my mind or The news knocks me for a loop','curve'],\n",
    "['modern','I stopped listening to modern music','current'],\n",
    "['mob','The police soon reduced the mob to order','crowd'],\n",
    "['impatient','Do not fuss about it so much or Do not be so impatient','cross'],\n",
    "['animal','Do you know what year of the animal you were born','creature'],\n",
    "['accident','The last thing I need is an accident','crash'],\n",
    "['cattle','The farmer is leading the cattle','cows'],\n",
    "['shelter','They sought shelter at my house','cover'],\n",
    "['sofa','How much does this sofa cost','couch'],\n",
    "['right','Drinking is all right as long as you do not do it to excess','correct'],\n",
    "['imitate','Speak naturally; do not try to imitate some actor','copy'],\n",
    "['vehicle','What is the license number of the vehicle','conveyance'],\n",
    "['carry','I always carry my important papers on my person','convey'],\n",
    "['compare','Shall I compare thee to a summers day','contrast'],\n",
    "['satisfy','Study hard to satisfy the examiners','content'],\n",
    "['pollution','Pollution will be a very big problem in the future','contamination'],\n",
    "['box','I want to send this box by third class mail','container'],\n",
    "['pot','Boil a lot of water in a pot','container'],\n",
    "['communicate','I use music to communicate with them','contact'],\n",
    "['communication','Computers are widely used as communication tools','contact'],\n",
    "['contemplate','All day he did nothing but contemplate','consider'],\n",
    "['gather','You can gather a lot if you scrape the barrel','congregate'],\n",
    "['agree','I am afraid I do not agree with that','concur'],\n",
    "['understand','I understand you re leaving us at the end of the month','comprehend'],\n",
    "['fraternity','The fraternity may bid five new men','companionship'],\n",
    "['friend','One student split on his friend','companion'],\n",
    "['arrive','Be sure you arrive at work on time every morning','come'],\n",
    "['tower','They are timing a race up the tower','column'],\n",
    "['clothes','His clothes are covered with crud from working on his car','clothing'],\n",
    "['priest','The priest talked about love for one another in his homily','clergyman'],\n",
    "['baby','An extra member was added to his family; he adopted a baby','child'],\n",
    "['smart','Buying a clunker like that was not such a smart thing to do','chic'],\n",
    "['verify','Use control charts to verify stability','check'],\n",
    "['inexpensive','Maybe we will find some really inexpensive clothes','cheap'],\n",
    "['fee','I put into any money to pay for your tuition fee','charge'],\n",
    "['tax','A heavy commodity tax is levied on gasoline','charge'],\n",
    "['room','A group of people packed in to the room','chamber'],\n",
    "['confident','Do not be too confident of yourself','certain'],\n",
    "['hole','The carpenter bored out a hole through a thick board','cavity'],\n",
    "['create','Huge fans are used to create strong winds','cause'],\n",
    "['money','Every moment is precious or Time is money','cash'],\n",
    "['marijuana','We can smoke marijuana any time in Vietnam','cannabis'],\n",
    "['cottage','Love in a cottage is also included in a happy life','cabin'],\n",
    "['corporation','What is true of the Deebay Shippin Corporation','business'],\n",
    "['insect','It eats the insect at a later time','bug'],\n",
    "['violent','Do not ever resort to violent means','brutal'],\n",
    "['deliver','Sports can also deliver the addictive qualities of a drug','bring'],\n",
    "['short','Two years is not a short period','brief'],\n",
    "['nerve','Eggs are helpful for people who have nerve damage','bravery'],\n",
    "['employer','The employer eats his workers alive','boss'],\n",
    "['dull','The party was a dull affair','boring'],\n",
    "['flower','Youth is the flower of life or Youth is a treasure','bloom'],\n",
    "['large','We are currently working on a large scale project','big'],\n",
    "['curve','He used to fog it in but now he focuses on a curve ball','bend'],\n",
    "['stomach','You cannot work on an empty stomach','belly'],\n",
    "['think','I am afraid not or I do not think so','believe'],\n",
    "['opinion','That is a matter of opinion','belief'],\n",
    "['presence','What could be considered evidence of a spirits presence','being'],\n",
    "['bed','There is a sick bed within the enclosure of this school','bedstead'],\n",
    "['get','The costs of college get higher every year','become'],\n",
    "['rhythm','The rhythm of the music quickens','beat'],\n",
    "['shoulder','lay hand upon a shoulder','bear'],\n",
    "['ray','There is still a ray of hope for his recovery','beam'],\n",
    "['shore','I lived near the shore last year','beach'],\n",
    "['bowl','Fill the second bowl with warm water','basin'],\n",
    "['song','Her first number was a song from a popular musical','ballad'],\n",
    "['medium','Kim stroke a happy medium at the debate competition','average'],\n",
    "['beauty','That car is a real beauty','attractiveness'],\n",
    "['beautiful','Boston is so beautiful this time of year','attractive'],\n",
    "['effort','The game needs some effort to master','attempt'],\n",
    "['try','Lets try the same time but next week Okay','attempt'],\n",
    "['club','expel a member from a club','association'],\n",
    "['helper','Tells the helper application to quit','assistant'],\n",
    "['appoint','I will ask the emperor to appoint you in my place','assign'],\n",
    "['guilty','Do not decide whether he is innocent or guilty yet','ashamed'],\n",
    "['organize','It is a mess you need to organize your ideas better','arrange'],\n",
    "['come','Not anymore Two more people want to come','approach'],\n",
    "['machine','The man left his machine in the street','appliance'],\n",
    "['plead','plead with a creditor for a longer time','appeal'],\n",
    "['proclaim','Proclaim liberty throughout all the land','announce'],\n",
    "['beast','A horse was used as a beast of draft','animal'],\n",
    "['fun','The party is a lot of fun','amusement'],\n",
    "['friendly','I am on friendly terms with him','amiable'],\n",
    "['liquor','It is not good to mix different kinds of liquor','alcohol'],\n",
    "['bubble','Look at the table in the bubble','air ball'],\n",
    "['goal','I feel my way around achieving my goal','aim'],\n",
    "['pact','The peace pact brought the war to an end','agreement'],\n",
    "['old','They spend more time online than 17 year old boys do','aged'],\n",
    "['era','The Jiang era does not begin in normal times','age'],\n",
    "['attach','I attach no importance to what he says','affix'],\n",
    "['determine','How burglars determine the best time to rob a home','affect'],\n",
    "['plane','Their plane will arrive first thing in the morning','aeroplane'],\n",
    "['acknowledge','Please acknowledge receipt of this document by signing','admit'],\n",
    "['fresh','As usual she looked fresh and full of energy','additional'],\n",
    "['activity','The office was a beehive of activity','action'],\n",
    "['behave','Do not behave like a hog','act'],\n",
    "['achieve','There is no easy way to achieve the goal','accomplish'],\n",
    "['school','He was a year ahead of me in school','academy'],\n",
    "['capability','I am sorry but this work is above my capability','ability'],\n",
    "['competence','The police has competence over that state','ability'],\n",
    "['achieve','There is no easy way to achieve aim','accomplish'],\n",
    "['school','He was a year ahead of me in school','academy'],\n",
    "['capability','I am sorry but this work is above my capability','ability'],\n",
    "['competence','The police has competence over that state','ability']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec ...........................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "\n",
      "gloVe ...........................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "\n",
      "word2vec + rnn ...........................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "\n",
      "gloVe +rnn ...........................................................................................................................................................................................................................................................................................................................................................................................................................................Average Rank for word2Vec: 14.815\n",
      "Average Rank for glove: 16.138\n",
      "Average Rank for word2vec + rnn: 15.595\n",
      "Average Rank for glove + rnn: 16.377\n"
     ]
    }
   ],
   "source": [
    "#Supress default INFO logging\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "n_candidate = 15\n",
    "\n",
    "print(\"word2vec \", end='')\n",
    "avg_rank_word2vec = 0\n",
    "for a in simlex_synonyms:\n",
    "    print('.', end='')\n",
    "    word2vec_candidates = w2v.similar_by_vector(a[0], n_candidate)\n",
    "    cnt = 1\n",
    "    found = False\n",
    "    for b in word2vec_candidates:\n",
    "        if b[0] == a[2]:\n",
    "            found = True\n",
    "            avg_rank_word2vec += cnt\n",
    "        cnt +=1\n",
    "    if found == False:\n",
    "        avg_rank_word2vec += 20\n",
    "avg_rank_word2vec = avg_rank_word2vec / len(simlex_synonyms)\n",
    "\n",
    "print(\"gloVe \", end='')\n",
    "avg_rank_glove = 0\n",
    "for a in simlex_synonyms:\n",
    "    print('.', end=''),\n",
    "    glove_candidates = glove.similar_by_vector(a[0], n_candidate)\n",
    "    cnt = 1\n",
    "    found = False\n",
    "    for b in glove_candidates:\n",
    "        if b[0] == a[2]:\n",
    "            found = True\n",
    "            avg_rank_glove += cnt\n",
    "        cnt +=1\n",
    "    if found == False:\n",
    "        avg_rank_glove += 20\n",
    "avg_rank_glove = avg_rank_glove / len(simlex_synonyms)\n",
    "\n",
    "print(\"word2vec + rnn \", end='')\n",
    "avg_rank_word2vec_rnn = 0\n",
    "sentences_word2vec = []\n",
    "for a in simlex_synonyms:\n",
    "    print('.', end=''),\n",
    "    word2vec_candidates = w2v.similar_by_vector(a[0], n_candidate)\n",
    "    for i in word2vec_candidates:\n",
    "        sentences_word2vec.append(a[1].replace(a[0], i[0]))\n",
    "    word2vec_rnn_candidates = load_and_score([s.split() for s in sentences_word2vec], sort=True)\n",
    "    cnt = 1\n",
    "    found = False\n",
    "    for b in word2vec_rnn_candidates:\n",
    "        if a[2] in b[1]:\n",
    "            found = True\n",
    "            avg_rank_word2vec_rnn += cnt\n",
    "            break\n",
    "        cnt +=1\n",
    "    if found == False:\n",
    "        avg_rank_word2vec_rnn += 20\n",
    "    word2vec_rnn_candidates = []\n",
    "    sentences_word2vec = []\n",
    "avg_rank_word2vec_rnn = avg_rank_word2vec_rnn / len(simlex_synonyms)\n",
    "\n",
    "print(\"gloVe +rnn \", end='')\n",
    "avg_rank_glove_rnn = 0\n",
    "sentences_glove = []\n",
    "for a in simlex_synonyms:\n",
    "    print('.', end=''),\n",
    "    glove_candidates = glove.similar_by_vector(a[0], n_candidate)\n",
    "    for i in glove_candidates:\n",
    "        sentences_glove.append(a[1].replace(a[0], i[0]))\n",
    "    glove_rnn_candidates = load_and_score([s.split() for s in sentences_glove], sort=True)\n",
    "    cnt = 1\n",
    "    found = False\n",
    "    for b in glove_rnn_candidates:\n",
    "        if a[2] in b[1]:\n",
    "            found = True\n",
    "            avg_rank_glove_rnn += cnt\n",
    "            break\n",
    "        cnt +=1\n",
    "    if found == False:\n",
    "        avg_rank_glove_rnn += 20\n",
    "    glove_rnn_candidates = []\n",
    "    sentences_glove = []\n",
    "avg_rank_glove_rnn = avg_rank_glove_rnn / len(simlex_synonyms)\n",
    "\n",
    "print(\"Average Rank for word2Vec:\", round(avg_rank_word2vec,3))\n",
    "print(\"Average Rank for glove:\", round(avg_rank_glove,3))\n",
    "print(\"Average Rank for word2vec + rnn:\", round(avg_rank_word2vec_rnn,3))\n",
    "print(\"Average Rank for glove + rnn:\", round(avg_rank_glove_rnn,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
