{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantically related words using pre-trained embeddings\n",
    "\n",
    "In this notebook, pre-trained word embeddings using word2vec on google news corpus or GloVe on Twitter data is utilized to arrive at synsets (synomyms sets) that are words with similar meanings.\n",
    "\n",
    "**Gensim word2vec APIs**: https://radimrehurek.com/gensim/models/word2vec.html\n",
    "\n",
    "**Pre-trained word2vec model on google news**: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing\n",
    "\n",
    "**Pre-trained GloVe model on Twitter 2B tweets**: https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "The above models are in the form of binary/text files that can be loaded into the environment at runtime.\n",
    "\n",
    "*Notebook runtime: ~ 4 mins*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim                     # implements word2vec model infrastructure and provides interfacing APIs \n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.test.utils import get_tmpfile\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained word2vec model\n",
    "word2vec_vectors = '../pretrained/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "w2v = gensim.models.KeyedVectors.load_word2vec_format(word2vec_vectors, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained GloVe model\n",
    "glove_vectors = '../pretrained/glove.twitter.27B.200d.txt'\n",
    "tmp_file = get_tmpfile(\"test_word2vec.txt\")\n",
    "\n",
    "glove2word2vec(glove_input_file=glove_vectors, word2vec_output_file=tmp_file)\n",
    "glove = gensim.models.KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec cosine similarity of ['minor', 'small']: 0.3416362702846527\n",
      "word2vec cosine similarity of ['minor', 'major']: 0.47539088129997253\n",
      "\n",
      "GloVe cosine similarity of ['minor', 'small']: 0.42706066370010376\n",
      "GloVe cosine similarity of ['minor', 'major']: 0.703789472579956\n"
     ]
    }
   ],
   "source": [
    "# similarity \n",
    "pair1 = ['minor','small']\n",
    "pair2 = ['minor','major']\n",
    "cos_dist1_w = w2v.similarity(pair1[0], pair1[1])\n",
    "cos_dist2_w = w2v.similarity(pair2[0], pair2[1])\n",
    "\n",
    "print('word2vec cosine similarity of {}: {}'.format(pair1, cos_dist1_w) )\n",
    "print('word2vec cosine similarity of {}: {}'.format(pair2, cos_dist2_w) )\n",
    "\n",
    "cos_dist1_g = glove.similarity(pair1[0], pair1[1])\n",
    "cos_dist2_g = glove.similarity(pair2[0], pair2[1])\n",
    "\n",
    "print('\\nGloVe cosine similarity of {}: {}'.format(pair1, cos_dist1_g) )\n",
    "print('GloVe cosine similarity of {}: {}'.format(pair2, cos_dist2_g) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem above is that similarity doesn't always translate to synonyms - the target word 'minor' is closer to 'major' than to 'small'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec Vector embedding dimension:  (300,)\n",
      "\n",
      "Printing a subset of the whole vector for the word 'minor':\n",
      "[ 0.06640625 -0.00228882  0.00402832 -0.28710938 -0.21972656  0.34765625\n",
      " -0.00494385 -0.01757812  0.12988281 -0.15917969 -0.15527344 -0.16992188\n",
      "  0.06933594 -0.14257812 -0.07958984  0.16992188  0.12109375  0.125\n",
      " -0.06494141]\n",
      "\n",
      "GloVe vector embedding dimension:  (200,)\n",
      "\n",
      "Printing a subset of the whole vector for the word 'minor':\n",
      "[-0.81609   -0.10689   -0.53273   -0.20412   -0.37599    0.12386\n",
      " -0.12322   -0.80024   -0.017576   0.30317   -0.068888  -1.0975\n",
      " -0.56645    0.37651   -0.46615   -0.42359   -0.076921  -0.012701\n",
      " -0.0067806]\n"
     ]
    }
   ],
   "source": [
    "# vector representation of the word\n",
    "vec_pair1_0_w = w2v.get_vector(pair1[0])\n",
    "print(\"word2vec Vector embedding dimension: \",vec_pair1_0_w.shape)\n",
    "print(\"\\nPrinting a subset of the whole vector for the word '{}':\".format(pair1[0]))\n",
    "print(vec_pair1_0_w[1:20])\n",
    "\n",
    "vec_pair1_0_g = glove.get_vector(pair1[0])\n",
    "print(\"\\nGloVe vector embedding dimension: \",vec_pair1_0_g.shape)\n",
    "print(\"\\nPrinting a subset of the whole vector for the word '{}':\".format(pair1[0]))\n",
    "print(vec_pair1_0_g[1:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar 15 words (by word) for 'major' by word2vec model:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('biggest', 0.657293975353241),\n",
       " ('significant', 0.619140088558197),\n",
       " ('big', 0.6057686805725098),\n",
       " ('main', 0.5380213856697083),\n",
       " ('key', 0.5354758501052856),\n",
       " ('huge', 0.5329675674438477),\n",
       " ('signficant', 0.5157025456428528),\n",
       " ('amajor', 0.49914824962615967),\n",
       " ('largest', 0.49542921781539917),\n",
       " ('greatest', 0.49444860219955444),\n",
       " ('Major', 0.4887048006057739),\n",
       " ('massive', 0.4786103069782257),\n",
       " ('minor', 0.47539088129997253),\n",
       " ('substantial', 0.46729937195777893),\n",
       " ('monumental', 0.46554115414619446)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most similar 15 words (by word) for 'major' by GloVe model:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('minor', 0.7037895321846008),\n",
       " ('huge', 0.6762630939483643),\n",
       " ('massive', 0.655586838722229),\n",
       " ('big', 0.6330057382583618),\n",
       " ('biggest', 0.6215412020683289),\n",
       " ('another', 0.6144845485687256),\n",
       " ('third', 0.6137520670890808),\n",
       " ('any', 0.6084322333335876),\n",
       " ('serious', 0.6081491112709045),\n",
       " ('issues', 0.6023706197738647),\n",
       " ('first', 0.5963584780693054),\n",
       " ('having', 0.5878738164901733),\n",
       " ('two', 0.5866069197654724),\n",
       " ('other', 0.5805503129959106),\n",
       " ('many', 0.5805253982543945)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# most similar words - by word\n",
    "n_similar = 15\n",
    "thisWord = 'major'\n",
    "\n",
    "print(\"Most similar {} words (by word) for '{}' by word2vec model:\".format(n_similar, thisWord))\n",
    "display(w2v.similar_by_word(thisWord, n_similar))\n",
    "print(\"\\nMost similar {} words (by word) for '{}' by GloVe model:\".format(n_similar, thisWord))\n",
    "display(glove.similar_by_word(thisWord, n_similar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the list of similar words returned by the model is different between the word2vec and GloVe models.\n",
    "\n",
    "This is expected as these two pre-trained models have different source corpus.\n",
    "This variety can be utilized to capture more 'potential' candidates, but at the same time, it also burdens the next step to screen out the less relevant ones. \n",
    "\n",
    "Maybe we could utilize the **APSyn/APSynP** for a decisive similarity metric. Another approach would be to rule out outliers using the outlier detection techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar 15 words (by vector) for 'major' by word2vec model:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('biggest', 0.657293975353241),\n",
       " ('significant', 0.619140088558197),\n",
       " ('big', 0.6057686805725098),\n",
       " ('main', 0.5380213856697083),\n",
       " ('key', 0.5354758501052856),\n",
       " ('huge', 0.5329675674438477),\n",
       " ('signficant', 0.5157025456428528),\n",
       " ('amajor', 0.49914824962615967),\n",
       " ('largest', 0.49542921781539917),\n",
       " ('greatest', 0.49444860219955444),\n",
       " ('Major', 0.4887048006057739),\n",
       " ('massive', 0.4786103069782257),\n",
       " ('minor', 0.47539088129997253),\n",
       " ('substantial', 0.46729937195777893),\n",
       " ('monumental', 0.46554115414619446)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most similar 15 words (by vector) for 'major' by GloVe model:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('minor', 0.7037895321846008),\n",
       " ('huge', 0.6762630939483643),\n",
       " ('massive', 0.655586838722229),\n",
       " ('big', 0.6330057382583618),\n",
       " ('biggest', 0.6215412020683289),\n",
       " ('another', 0.6144845485687256),\n",
       " ('third', 0.6137520670890808),\n",
       " ('any', 0.6084322333335876),\n",
       " ('serious', 0.6081491112709045),\n",
       " ('issues', 0.6023706197738647),\n",
       " ('first', 0.5963584780693054),\n",
       " ('having', 0.5878738164901733),\n",
       " ('two', 0.5866069197654724),\n",
       " ('other', 0.5805503129959106),\n",
       " ('many', 0.5805253982543945)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# most similar words - by vector\n",
    "print(\"Most similar {} words (by vector) for '{}' by word2vec model:\".format(n_similar, thisWord))\n",
    "display(w2v.similar_by_vector(thisWord, n_similar))\n",
    "print(\"\\nMost similar {} words (by vector) for '{}' by GloVe model:\".format(n_similar, thisWord))\n",
    "display(glove.similar_by_vector(thisWord, n_similar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One analysis to be done is to evaluate similar words returned *by word* contrasted with *by vector* metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps:\n",
    "- Try other forms of embeddings that can improve upon word2vec e.g. \n",
    "    + GloVe\n",
    "    - fastText \n",
    "- Inspect the performace across less frequent words (fastText should perform better in this scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other resources\n",
    "- http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/\n",
    "- https://www.quora.com/Where-can-I-find-some-pre-trained-word-vectors-for-natural-language-processing-understanding\n",
    "- https://textminingonline.com/getting-started-with-word2vec-and-glove-in-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
