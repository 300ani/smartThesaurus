{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'rnn.rnnlm_test' from '/Users/jeylee/MIDS/W266/smartThesaurus/Code/rnn/rnnlm_test.py'>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim                     # implements word2vec model infrastructure and provides interfacing APIs \n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.test.utils import get_tmpfile\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import json, os, re, shutil, sys, time\n",
    "from importlib import reload\n",
    "import collections, itertools\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "# Helper libraries\n",
    "from rnn.w266_common import utils, vocabulary, tf_embed_viz\n",
    "\n",
    "# Your code\n",
    "import rnn.rnnlm as rnnlm;reload(rnnlm)\n",
    "import rnn.rnnlm_test as rnnlm_test;reload(rnnlm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained word2vec model\n",
    "word2vec_vectors = '../pretrained/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "w2v = gensim.models.KeyedVectors.load_word2vec_format(word2vec_vectors, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load pre-trained GloVe model\n",
    "glove_vectors = '../pretrained/glove.twitter.27B.200d.txt'\n",
    "tmp_file = get_tmpfile(\"test_word2vec.txt\")\n",
    "\n",
    "glove2word2vec(glove_input_file=glove_vectors, word2vec_output_file=tmp_file)\n",
    "glove = gensim.models.KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec cosine similarity of ['minor', 'small']: 0.3416362404823303\n",
      "word2vec cosine similarity of ['minor', 'major']: 0.47539088129997253\n",
      "\n",
      "GloVe cosine similarity of ['minor', 'small']: 0.42706066370010376\n",
      "GloVe cosine similarity of ['minor', 'major']: 0.7037895321846008\n"
     ]
    }
   ],
   "source": [
    "# similarity \n",
    "pair1 = ['minor','small']\n",
    "pair2 = ['minor','major']\n",
    "cos_dist1_w = w2v.similarity(pair1[0], pair1[1])\n",
    "cos_dist2_w = w2v.similarity(pair2[0], pair2[1])\n",
    "\n",
    "print('word2vec cosine similarity of {}: {}'.format(pair1, cos_dist1_w) )\n",
    "print('word2vec cosine similarity of {}: {}'.format(pair2, cos_dist2_w) )\n",
    "\n",
    "cos_dist1_g = glove.similarity(pair1[0], pair1[1])\n",
    "cos_dist2_g = glove.similarity(pair2[0], pair2[1])\n",
    "\n",
    "print('\\nGloVe cosine similarity of {}: {}'.format(pair1, cos_dist1_g) )\n",
    "print('GloVe cosine similarity of {}: {}'.format(pair2, cos_dist2_g) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec Vector embedding dimension:  (300,)\n",
      "\n",
      "Printing a subset of the whole vector for the word 'minor':\n",
      "[ 0.06640625 -0.00228882  0.00402832 -0.28710938 -0.21972656  0.34765625\n",
      " -0.00494385 -0.01757812  0.12988281 -0.15917969 -0.15527344 -0.16992188\n",
      "  0.06933594 -0.14257812 -0.07958984  0.16992188  0.12109375  0.125\n",
      " -0.06494141]\n",
      "\n",
      "GloVe vector embedding dimension:  (200,)\n",
      "\n",
      "Printing a subset of the whole vector for the word 'minor':\n",
      "[-0.81608999 -0.10689    -0.53272998 -0.20412    -0.37599     0.12386\n",
      " -0.12322    -0.80023998 -0.017576    0.30317    -0.068888   -1.09749997\n",
      " -0.56645     0.37650999 -0.46614999 -0.42359    -0.076921   -0.012701\n",
      " -0.0067806 ]\n"
     ]
    }
   ],
   "source": [
    "# vector representation of the word\n",
    "vec_pair1_0_w = w2v.get_vector(pair1[0])\n",
    "print(\"word2vec Vector embedding dimension: \",vec_pair1_0_w.shape)\n",
    "print(\"\\nPrinting a subset of the whole vector for the word '{}':\".format(pair1[0]))\n",
    "print(vec_pair1_0_w[1:20])\n",
    "\n",
    "vec_pair1_0_g = glove.get_vector(pair1[0])\n",
    "print(\"\\nGloVe vector embedding dimension: \",vec_pair1_0_g.shape)\n",
    "print(\"\\nPrinting a subset of the whole vector for the word '{}':\".format(pair1[0]))\n",
    "print(vec_pair1_0_g[1:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar 15 words (by word) for 'major' by word2vec model:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('biggest', 0.6572940349578857),\n",
       " ('significant', 0.619140088558197),\n",
       " ('big', 0.6057686805725098),\n",
       " ('main', 0.5380213260650635),\n",
       " ('key', 0.5354758501052856),\n",
       " ('huge', 0.5329675674438477),\n",
       " ('signficant', 0.5157025456428528),\n",
       " ('amajor', 0.49914824962615967),\n",
       " ('largest', 0.49542921781539917),\n",
       " ('greatest', 0.49444860219955444),\n",
       " ('Major', 0.4887048304080963),\n",
       " ('massive', 0.4786102771759033),\n",
       " ('minor', 0.47539088129997253),\n",
       " ('substantial', 0.46729934215545654),\n",
       " ('monumental', 0.46554118394851685)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most similar 15 words (by word) for 'major' by GloVe model:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('minor', 0.703789472579956),\n",
       " ('huge', 0.6762630939483643),\n",
       " ('massive', 0.655586838722229),\n",
       " ('big', 0.6330057382583618),\n",
       " ('biggest', 0.6215412616729736),\n",
       " ('another', 0.6144846081733704),\n",
       " ('third', 0.6137520670890808),\n",
       " ('any', 0.6084322333335876),\n",
       " ('serious', 0.6081491112709045),\n",
       " ('issues', 0.6023705005645752),\n",
       " ('first', 0.5963584780693054),\n",
       " ('having', 0.5878738164901733),\n",
       " ('two', 0.5866069197654724),\n",
       " ('other', 0.5805503129959106),\n",
       " ('many', 0.5805253982543945)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# most similar words - by word\n",
    "n_similar = 15\n",
    "thisWord = 'major'\n",
    "\n",
    "print(\"Most similar {} words (by word) for '{}' by word2vec model:\".format(n_similar, thisWord))\n",
    "display(w2v.similar_by_word(thisWord, n_similar))\n",
    "print(\"\\nMost similar {} words (by word) for '{}' by GloVe model:\".format(n_similar, thisWord))\n",
    "display(glove.similar_by_word(thisWord, n_similar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_shapes_embed (rnn.rnnlm_test.TestRNNLMCore) ... ok\n",
      "test_shapes_output (rnn.rnnlm_test.TestRNNLMCore) ... ok\n",
      "test_shapes_recurrent (rnn.rnnlm_test.TestRNNLMCore) ... ok\n",
      "test_shapes_train (rnn.rnnlm_test.TestRNNLMTrain) ... ok\n",
      "test_shapes_sample (rnn.rnnlm_test.TestRNNLMSampler) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 2.049s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "reload(rnnlm)\n",
    "\n",
    "TF_GRAPHDIR = \"/Users/jeylee/mids/W266/tmp/w266/a3_graph\"\n",
    "\n",
    "# Clear old log directory.\n",
    "shutil.rmtree(TF_GRAPHDIR, ignore_errors=True)\n",
    "\n",
    "lm = rnnlm.RNNLM(V=10000, H=200, num_layers=2)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "lm.BuildSamplerGraph()\n",
    "\n",
    "summary_writer = tf.summary.FileWriter(TF_GRAPHDIR, lm.graph)\n",
    "\n",
    "reload(rnnlm); reload(rnnlm_test)\n",
    "utils.run_tests(rnnlm_test, [\"TestRNNLMCore\", \"TestRNNLMTrain\", \"TestRNNLMSampler\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=10, learning_rate=None):\n",
    "    assert(learning_rate is not None)\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step_\n",
    "        use_dropout = True\n",
    "        loss = lm.train_loss_\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "        loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "    for i, (w, y) in enumerate(batch_iterator):\n",
    "        # At first batch in epoch, get a clean intitial state.\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "\n",
    "        feed_dict = {\n",
    "            lm.input_w_: w,\n",
    "            lm.target_y_: y,\n",
    "            lm.initial_h_: h,\n",
    "            lm.learning_rate_: learning_rate,\n",
    "            lm.use_dropout_: use_dropout\n",
    "        }\n",
    "        ops = [loss, lm.final_h_, train_op]        \n",
    "        #### YOUR CODE HERE ####\n",
    "        # session.run(...) the ops with the feed_dict constructed above.\n",
    "        # Ensure \"cost\" becomes the value of \"loss\".\n",
    "        # Hint: see \"ops\" for other variables that need updating in this loop.\n",
    "        \n",
    "        cost, h, _ = session.run([loss, lm.final_h_ ,train_op], feed_dict=feed_dict)\n",
    "        \n",
    "        #### END(YOUR CODE) ####\n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "        ##\n",
    "        # Print average loss-so-far for epoch\n",
    "        # If using train_loss_, this may be an underestimate.\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print(\"[batch {:d}]: seen {:d} words at {:.1f} wps, loss = {:.3f}\".format(\n",
    "                i, total_words, avg_wps, avg_cost))\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches\n",
    "\n",
    "def score_dataset(lm, session, ids, name=\"Data\"):\n",
    "    # For scoring, we can use larger batches to speed things up.\n",
    "    bi = utils.rnnlm_batch_generator(ids, batch_size=100, max_time=100)\n",
    "    cost = run_epoch(lm, session, bi, \n",
    "                     learning_rate=0.0, train=False, \n",
    "                     verbose=False, tick_s=3600)\n",
    "    print(\"{:s}: avg. loss: {:.03f}  (perplexity: {:.02f})\".format(name, cost, np.exp(cost)))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_toy_model (rnn.rnnlm_test.RunEpochTester) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 143]: seen 7200 words at 7149.5 wps, loss = 0.638\n",
      "[batch 307]: seen 15400 words at 7655.6 wps, loss = 0.392\n",
      "[batch 469]: seen 23500 words at 7799.3 wps, loss = 0.296\n",
      "[batch 634]: seen 31750 words at 7906.6 wps, loss = 0.245\n",
      "[batch 788]: seen 39450 words at 7863.2 wps, loss = 0.215\n",
      "[batch 949]: seen 47500 words at 7890.5 wps, loss = 0.193\n",
      "[batch 1112]: seen 55650 words at 7919.1 wps, loss = 0.176\n",
      "[batch 1255]: seen 62800 words at 7817.2 wps, loss = 0.166\n",
      "[batch 1417]: seen 70900 words at 7847.5 wps, loss = 0.154\n",
      "[batch 1580]: seen 79050 words at 7876.9 wps, loss = 0.145\n",
      "[batch 1744]: seen 87250 words at 7904.1 wps, loss = 0.138\n",
      "[batch 1909]: seen 95500 words at 7929.0 wps, loss = 0.132\n",
      "[batch 2074]: seen 103750 words at 7953.1 wps, loss = 0.126\n",
      "[batch 2238]: seen 111950 words at 7969.7 wps, loss = 0.122\n",
      "[batch 2402]: seen 120150 words at 7983.0 wps, loss = 0.117\n",
      "[batch 2566]: seen 128350 words at 7995.7 wps, loss = 0.114\n",
      "[batch 2730]: seen 136550 words at 8005.7 wps, loss = 0.111\n",
      "[batch 2893]: seen 144700 words at 8012.0 wps, loss = 0.108\n",
      "[batch 3057]: seen 152900 words at 8019.3 wps, loss = 0.106\n",
      "[batch 3222]: seen 161150 words at 8029.0 wps, loss = 0.103\n",
      "[batch 3387]: seen 169400 words at 8037.3 wps, loss = 0.101\n",
      "[batch 3550]: seen 177550 words at 8041.5 wps, loss = 0.099\n",
      "[batch 3709]: seen 185500 words at 8036.2 wps, loss = 0.097\n",
      "[batch 3873]: seen 193700 words at 8041.6 wps, loss = 0.096\n",
      "Train set: avg. loss: 0.000  (perplexity: 1.00)\n",
      "Test set: avg. loss: 0.014  (perplexity: 1.01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 26.875s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(rnnlm); reload(rnnlm_test)\n",
    "th = rnnlm_test.RunEpochTester(\"test_toy_model\")\n",
    "th.setUp(); th.injectCode(run_epoch, score_dataset)\n",
    "unittest.TextTestRunner(verbosity=2).run(th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/jeylee/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "Vocabulary: 10,000 types\n",
      "Loaded 57,340 sentences (1.16119e+06 tokens)\n",
      "Training set: 45,872 sentences (924,077 tokens)\n",
      "Test set: 11,468 sentences (237,115 tokens)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "V = 10000\n",
    "vocab, train_ids, test_ids = utils.load_corpus(\"brown\", split=0.8, V=V, shuffle=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "max_time = 25\n",
    "batch_size = 100\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(V=vocab.size, \n",
    "                    H=200, \n",
    "                    softmax_ns=200,\n",
    "                    num_layers=2)\n",
    "\n",
    "TF_SAVEDIR = \"/Users/jeylee/mids/W266/tmp/w266/a3_model\"\n",
    "checkpoint_filename = os.path.join(TF_SAVEDIR, \"rnnlm\")\n",
    "trained_filename = os.path.join(TF_SAVEDIR, \"rnnlm_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] Starting epoch 1\n",
      "[epoch 1] Completed in 0:01:37\n",
      "[epoch 1] Train set: avg. loss: 5.396  (perplexity: 220.47)\n",
      "[epoch 1] Test set: avg. loss: 5.434  (perplexity: 229.02)\n",
      "\n",
      "[epoch 2] Starting epoch 2\n",
      "[epoch 2] Completed in 0:01:42\n",
      "[epoch 2] Train set: avg. loss: 5.219  (perplexity: 184.80)\n",
      "[epoch 2] Test set: avg. loss: 5.275  (perplexity: 195.45)\n",
      "\n",
      "[epoch 3] Starting epoch 3\n",
      "[epoch 3] Completed in 0:01:50\n",
      "[epoch 3] Train set: avg. loss: 5.103  (perplexity: 164.56)\n",
      "[epoch 3] Test set: avg. loss: 5.173  (perplexity: 176.39)\n",
      "\n",
      "[epoch 4] Starting epoch 4\n",
      "[epoch 4] Completed in 0:02:15\n",
      "[epoch 4] Train set: avg. loss: 5.030  (perplexity: 152.97)\n",
      "[epoch 4] Test set: avg. loss: 5.112  (perplexity: 165.95)\n",
      "\n",
      "[epoch 5] Starting epoch 5\n",
      "[epoch 5] Completed in 0:01:48\n",
      "[epoch 5] Train set: avg. loss: 4.970  (perplexity: 143.96)\n",
      "[epoch 5] Test set: avg. loss: 5.064  (perplexity: 158.21)\n",
      "\n",
      "[epoch 6] Starting epoch 6\n",
      "[epoch 6] Completed in 0:01:40\n",
      "[epoch 6] Train set: avg. loss: 4.926  (perplexity: 137.83)\n",
      "[epoch 6] Test set: avg. loss: 5.031  (perplexity: 153.07)\n",
      "\n",
      "[epoch 7] Starting epoch 7\n",
      "[epoch 7] Completed in 0:01:56\n",
      "[epoch 7] Train set: avg. loss: 4.892  (perplexity: 133.16)\n",
      "[epoch 7] Test set: avg. loss: 5.007  (perplexity: 149.47)\n",
      "\n",
      "[epoch 8] Starting epoch 8\n",
      "[epoch 8] Completed in 0:01:57\n",
      "[epoch 8] Train set: avg. loss: 4.864  (perplexity: 129.57)\n",
      "[epoch 8] Test set: avg. loss: 4.988  (perplexity: 146.70)\n",
      "\n",
      "[epoch 9] Starting epoch 9\n",
      "[epoch 9] Completed in 0:02:01\n",
      "[epoch 9] Train set: avg. loss: 4.833  (perplexity: 125.56)\n",
      "[epoch 9] Test set: avg. loss: 4.967  (perplexity: 143.61)\n",
      "\n",
      "[epoch 10] Starting epoch 10\n",
      "[epoch 10] Completed in 0:01:43\n",
      "[epoch 10] Train set: avg. loss: 4.810  (perplexity: 122.77)\n",
      "[epoch 10] Test set: avg. loss: 4.953  (perplexity: 141.66)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Will print status every this many seconds\n",
    "print_interval = 5\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "\n",
    "# Explicitly add global initializer and variable saver to LM graph\n",
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "if not os.path.isdir(TF_SAVEDIR):\n",
    "    os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    session.run(initializer)\n",
    "\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        bi = utils.rnnlm_batch_generator(train_ids, batch_size, max_time)\n",
    "        print(\"[epoch {:d}] Starting epoch {:d}\".format(epoch, epoch))\n",
    "        #### YOUR CODE HERE ####\n",
    "        # Run a training epoch.\n",
    "        \n",
    "        run_epoch(lm, session, bi, learning_rate=learning_rate, train=True, verbose=False, tick_s=10)\n",
    "        \n",
    "        #### END(YOUR CODE) ####\n",
    "        print(\"[epoch {:d}] Completed in {:s}\".format(epoch, utils.pretty_timedelta(since=t0_epoch)))\n",
    "    \n",
    "        # Save a checkpoint\n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "    \n",
    "        ##\n",
    "        # score_dataset will run a forward pass over the entire dataset\n",
    "        # and report perplexity scores. This can be slow (around 1/2 to \n",
    "        # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "        # to speed up training on a slow machine. Be sure to run it at the \n",
    "        # end to evaluate your score.\n",
    "        print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "        score_dataset(lm, session, train_ids, name=\"Train set\")\n",
    "        print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "        score_dataset(lm, session, test_ids, name=\"Test set\")\n",
    "        print(\"\")\n",
    "    \n",
    "    # Save final model\n",
    "    saver.save(session, trained_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_seq(lm, session, seq, vocab):\n",
    "    \"\"\"Score a sequence of words. Returns total log-probability.\"\"\"\n",
    "    padded_ids = vocab.words_to_ids(utils.canonicalize_words([\"<s>\"] + seq + [\"</s>\"], \n",
    "                                                             wordset=vocab.word_to_id))\n",
    "    w = np.reshape(padded_ids[:-1], [1,-1])\n",
    "    y = np.reshape(padded_ids[1:],  [1,-1])\n",
    "    h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "    feed_dict = {lm.input_w_:w,\n",
    "                 lm.target_y_:y,\n",
    "                 lm.initial_h_:h,\n",
    "                 lm.dropout_keep_prob_: 1.0}\n",
    "    # Return log(P(seq)) = -1*loss\n",
    "    return -1*session.run(lm.loss_, feed_dict)\n",
    "\n",
    "def load_and_score(inputs, sort=False):\n",
    "    \"\"\"Load the trained model and score the given words.\"\"\"\n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    \n",
    "    with lm.graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session(graph=lm.graph) as session:  \n",
    "        # Load the trained model\n",
    "        saver.restore(session, trained_filename)\n",
    "\n",
    "        if isinstance(inputs[0], str) or isinstance(inputs[0], bytes):\n",
    "            inputs = [inputs]\n",
    "\n",
    "        # Actually run scoring\n",
    "        results = []\n",
    "        for words in inputs:\n",
    "            score = score_seq(lm, session, words, vocab)\n",
    "            results.append((score, words))\n",
    "\n",
    "        # Sort if requested\n",
    "        if sort: results = sorted(results, reverse=True)\n",
    "\n",
    "        # Print results\n",
    "        for score, words in results:\n",
    "            print(\"\\\"{:s}\\\" : {:.02f}\".format(\" \".join(words), score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/jeylee/mids/W266/tmp/w266/a3_model/rnnlm_trained\n",
      "\"There's going to be major changes to the company next year.\" : -6.38\n"
     ]
    }
   ],
   "source": [
    "sents = [\"There's going to be major changes to the company next year.\"]\n",
    "load_and_score([s.split() for s in sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar 15 words (by vector) for 'major' by word2vec model:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('biggest', 0.6572940349578857),\n",
       " ('significant', 0.619140088558197),\n",
       " ('big', 0.6057686805725098),\n",
       " ('main', 0.5380213260650635),\n",
       " ('key', 0.5354758501052856),\n",
       " ('huge', 0.5329675674438477),\n",
       " ('signficant', 0.5157025456428528),\n",
       " ('amajor', 0.49914824962615967),\n",
       " ('largest', 0.49542921781539917),\n",
       " ('greatest', 0.49444860219955444),\n",
       " ('Major', 0.4887048304080963),\n",
       " ('massive', 0.4786102771759033),\n",
       " ('minor', 0.47539088129997253),\n",
       " ('substantial', 0.46729934215545654),\n",
       " ('monumental', 0.46554118394851685)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most similar 15 words (by vector) for 'major' by GloVe model:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('minor', 0.703789472579956),\n",
       " ('huge', 0.6762630939483643),\n",
       " ('massive', 0.655586838722229),\n",
       " ('big', 0.6330057382583618),\n",
       " ('biggest', 0.6215412616729736),\n",
       " ('another', 0.6144846081733704),\n",
       " ('third', 0.6137520670890808),\n",
       " ('any', 0.6084322333335876),\n",
       " ('serious', 0.6081491112709045),\n",
       " ('issues', 0.6023705005645752),\n",
       " ('first', 0.5963584780693054),\n",
       " ('having', 0.5878738164901733),\n",
       " ('two', 0.5866069197654724),\n",
       " ('other', 0.5805503129959106),\n",
       " ('many', 0.5805253982543945)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# most similar words - by vector\n",
    "print(\"Most similar {} words (by vector) for '{}' by word2vec model:\".format(n_similar, thisWord))\n",
    "display(w2v.similar_by_vector(thisWord, n_similar))\n",
    "print(\"\\nMost similar {} words (by vector) for '{}' by GloVe model:\".format(n_similar, thisWord))\n",
    "display(glove.similar_by_vector(thisWord, n_similar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Most similar 15 words (by vector) for 'major' by word2vec model:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('biggest', 0.6572940349578857),\n",
       " ('significant', 0.619140088558197),\n",
       " ('big', 0.6057686805725098),\n",
       " ('main', 0.5380213260650635),\n",
       " ('key', 0.5354758501052856),\n",
       " ('huge', 0.5329675674438477),\n",
       " ('signficant', 0.5157025456428528),\n",
       " ('amajor', 0.49914824962615967),\n",
       " ('largest', 0.49542921781539917),\n",
       " ('greatest', 0.49444860219955444),\n",
       " ('Major', 0.4887048304080963),\n",
       " ('massive', 0.4786102771759033),\n",
       " ('minor', 0.47539088129997253),\n",
       " ('substantial', 0.46729934215545654),\n",
       " ('monumental', 0.46554118394851685)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Most similar 15 words (by vector) for 'major' by GloVe model:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('minor', 0.703789472579956),\n",
       " ('huge', 0.6762630939483643),\n",
       " ('massive', 0.655586838722229),\n",
       " ('big', 0.6330057382583618),\n",
       " ('biggest', 0.6215412616729736),\n",
       " ('another', 0.6144846081733704),\n",
       " ('third', 0.6137520670890808),\n",
       " ('any', 0.6084322333335876),\n",
       " ('serious', 0.6081491112709045),\n",
       " ('issues', 0.6023705005645752),\n",
       " ('first', 0.5963584780693054),\n",
       " ('having', 0.5878738164901733),\n",
       " ('two', 0.5866069197654724),\n",
       " ('other', 0.5805503129959106),\n",
       " ('many', 0.5805253982543945)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Most similar 15 words for 'major' by word2vec + RNN model:\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from /Users/jeylee/mids/W266/tmp/w266/a3_model/rnnlm_trained\n",
      "\"There is going to be a signficant change in the company next year.\" : -4.70\n",
      "\"There is going to be a monumental change in the company next year.\" : -4.70\n",
      "\"There is going to be a amajor change in the company next year.\" : -4.70\n",
      "\"There is going to be a big change in the company next year.\" : -4.93\n",
      "\"There is going to be a Major change in the company next year.\" : -4.98\n",
      "\"There is going to be a huge change in the company next year.\" : -5.01\n",
      "\"There is going to be a significant change in the company next year.\" : -5.03\n",
      "\"There is going to be a substantial change in the company next year.\" : -5.03\n",
      "\"There is going to be a massive change in the company next year.\" : -5.06\n",
      "\"There is going to be a greatest change in the company next year.\" : -5.09\n",
      "\"There is going to be a main change in the company next year.\" : -5.10\n",
      "\"There is going to be a minor change in the company next year.\" : -5.14\n",
      "\"There is going to be a biggest change in the company next year.\" : -5.16\n",
      "\"There is going to be a largest change in the company next year.\" : -5.22\n",
      "\"There is going to be a key change in the company next year.\" : -5.22\n",
      "\n",
      "### Most similar 15 words for 'major' by GloVe + RNN model:\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from /Users/jeylee/mids/W266/tmp/w266/a3_model/rnnlm_trained\n",
      "\"There is going to be a big change in the company next year.\" : -4.93\n",
      "\"There is going to be a first change in the company next year.\" : -4.96\n",
      "\"There is going to be a third change in the company next year.\" : -5.00\n",
      "\"There is going to be a huge change in the company next year.\" : -5.01\n",
      "\"There is going to be a serious change in the company next year.\" : -5.01\n",
      "\"There is going to be a other change in the company next year.\" : -5.02\n",
      "\"There is going to be a massive change in the company next year.\" : -5.06\n",
      "\"There is going to be a two change in the company next year.\" : -5.09\n",
      "\"There is going to be a many change in the company next year.\" : -5.10\n",
      "\"There is going to be a minor change in the company next year.\" : -5.14\n",
      "\"There is going to be a biggest change in the company next year.\" : -5.16\n",
      "\"There is going to be a any change in the company next year.\" : -5.22\n",
      "\"There is going to be a another change in the company next year.\" : -5.26\n",
      "\"There is going to be a having change in the company next year.\" : -5.46\n",
      "\"There is going to be a issues change in the company next year.\" : -5.50\n"
     ]
    }
   ],
   "source": [
    "targetSentence = 'There is going to be a major change in the company next year.'\n",
    "targetWord = 'major'\n",
    "print(\"### Most similar {} words (by vector) for '{}' by word2vec model:\".format(n_similar, targetWord))\n",
    "display(w2v.similar_by_vector(targetWord, n_similar))\n",
    "print(\"\\n### Most similar {} words (by vector) for '{}' by GloVe model:\".format(n_similar, targetWord))\n",
    "display(glove.similar_by_vector(targetWord, n_similar))\n",
    "\n",
    "word2vec_candidates = w2v.similar_by_vector(targetWord, n_similar)\n",
    "glove_candidates = glove.similar_by_vector(targetWord, n_similar)\n",
    "\n",
    "print(\"\\n### Most similar {} words for '{}' by word2vec + RNN model:\\n\".format(n_similar, targetWord))\n",
    "sentences_word2vec = []\n",
    "for i in word2vec_candidates:\n",
    "    sentences_word2vec.append(targetSentence.replace('major', i[0]))\n",
    "word2vec_rnn_candidates = load_and_score([s.split() for s in sentences_word2vec], sort=True)\n",
    "\n",
    "print(\"\\n### Most similar {} words for '{}' by GloVe + RNN model:\\n\".format(n_similar, targetWord))\n",
    "sentences_glove = []\n",
    "for i in glove_candidates:\n",
    "    sentences_glove.append(targetSentence.replace('major', i[0]))\n",
    "glove_rnn_candidates = load_and_score([s.split() for s in sentences_glove], sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
